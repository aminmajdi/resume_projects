{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter POS Tagging \n",
    "The goal of this tutorial is to introduce a a Part-of-Speech (POS) tagger developed for tweets which was released as part of the [TweetNLP](https://www.ark.cs.cmu.edu/TweetNLP/) toolkit. The code is written in Java and the python wrapper for the tokenization is from [this](https://github.com/myleott/ark-twokenize-py) github repository. This tutorial has code from the [TweetNLP](https://github.com/brendano/ark-tweet-nlp/) github repository as well as the python wrapper from [this](https://github.com/ianozsvald/ark-tweet-nlp-python) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "- POS tagging involves identifying part-of-speech of tokens in a given text. This can be viewed as a task of labeling the sentence w_1, w_2, ....., w_n with pos tags, one for each word: t_1, t_2, ...., t_n.\n",
    "- The 8 common parts of speech for english language are:\n",
    "  1. Noun\n",
    "  2. Verb\n",
    "  3. Pronoun\n",
    "  4. Preposition\n",
    "  5. Adverb\n",
    "  6. Conjuction\n",
    "  7. Participle\n",
    "  8. Article  \n",
    "- Twitter data is different from standard language data in that there are tokens such as #, @, emoticons, URLs, etc. So the tagset for twitter needs to incorporate the tags for these new tokens. The tags that are used to annotate tweets are as follows:\n",
    "\n",
    "<img src=\"pos_tags.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments\n",
    "- This tutorial covers how to accomplish the task of POS tagging for twitter data based on this paper: https://aclanthology.org/P11-2008.pdf\n",
    "- The nature of twitter data poses challenges in using standard POS taggers. The paper develops the above tagset for twitter to include tags for words that are not commonly encountered in language outside of twitter. \n",
    "- Around 1,800 tweets were manually annotated with corresponding pos tags.\n",
    "- Conditional Random Fields (CRFs) were used with features specific to twitter POS tagging. The features for the CRF are below (see paper for more details):\n",
    "  - Twitter orthography - these features are rules that detect @, #, and URls.\n",
    "  - Names - these features check for names from a dictionary of compiled tokens which are frequently capitalized.\n",
    "  - Traditional Tag Dictionary - these are features for all tags that occur in PTB.\n",
    "  - Distributional Similarity - these features are constructed from the successor and predecessor probabilities for the 10,000 most common terms.\n",
    "  - Phonetic normalization - words are normalized to ignore alternate spellings of words using the Metaphone algorithm; e.x.{thangs, thanks, thanksss, thanx, thinks, thnx} are mapped to 0NKS.\n",
    "- 1827 tweets that are annotated are divided into training set of 1000 tweets, dev set of 327 tweets, and test set of 500 tweets. The results of the tagger incorporating the above features are compared with the standard Stanford Tagger and using the above feature set for twitter data reduces error by about 25%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "- You will need to download the POS tagger from https://code.google.com/archive/p/ark-tweet-nlp/downloads\n",
    "- This requires Java 6. https://www.oracle.com/java/technologies/java-platform.html\n",
    "- Place this ipython notebook that has python wrappers inside the ark-tweet-nlp-0.3.2 folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "try:\n",
    "    from html.parser import HTMLParser\n",
    "except ImportError:\n",
    "    from HTMLParser import HTMLParser\n",
    "\n",
    "try:\n",
    "    import html\n",
    "except ImportError:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Contractions = re.compile(u\"(?i)(\\w+)(n['’′]t|['’′]ve|['’′]ll|['’′]d|['’′]re|['’′]s|['’′]m)$\", re.UNICODE)\n",
    "Whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
    "punctChars = r\"['\\\"“”‘’.?!…,:;]\"\n",
    "punctSeq   = r\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\n",
    "entity     = r\"&(?:amp|lt|gt|quot);\" # see more here https://www.w3schools.com/html/html_entities.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_or(*items):\n",
    "    return '(?:' + '|'.join(items) + ')'\n",
    "\n",
    "urlStart1  = r\"(?:https?://|\\bwww\\.)\"\n",
    "commonTLDs = r\"(?:com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xxx)\"\n",
    "ccTLDs = r\"(?:ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|\" + \\\n",
    "r\"bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|\" + \\\n",
    "r\"er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|\" + \\\n",
    "r\"hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|\" + \\\n",
    "r\"lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|\" + \\\n",
    "r\"nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|\" + \\\n",
    "r\"sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|\" + \\\n",
    "r\"va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)\"\t#TODO: remove obscure country domains?\n",
    "urlStart2  = r\"\\b(?:[A-Za-z\\d-])+(?:\\.[A-Za-z0-9]+){0,3}\\.\" + regex_or(commonTLDs, ccTLDs) + r\"(?:\\.\"+ccTLDs+r\")?(?=\\W|$)\"\n",
    "urlBody    = r\"(?:[^\\.\\s<>][^\\s<>]*?)?\"\n",
    "urlExtraCrapBeforeEnd = regex_or(punctChars, entity) + \"+?\"\n",
    "urlEnd     = r\"(?:\\.\\.+|[<>]|\\s|$)\"\n",
    "url        = regex_or(urlStart1, urlStart2) + urlBody + \"(?=(?:\"+urlExtraCrapBeforeEnd+\")?\"+urlEnd+\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "monetary = r\"\\$([0-9]+)?\\.?([0-9]+)?\"\n",
    "timeLike   = r\"\\d+(?::\\d+){1,2}\"\n",
    "numberWithCommas = r\"(?:(?<!\\d)\\d{1,3},)+?\\d{3}\" + r\"(?=(?:[^,\\d]|$))\"\n",
    "numComb = u\"[\\u0024\\u058f\\u060b\\u09f2\\u09f3\\u09fb\\u0af1\\u0bf9\\u0e3f\\u17db\\ua838\\ufdfc\\ufe69\\uff04\\uffe0\\uffe1\\uffe5\\uffe6\\u00a2-\\u00a5\\u20a0-\\u20b9]?\"\n",
    "boundaryNotDot = regex_or(\"$\", r\"\\s\", r\"[“\\\"?!,:;]\", entity)\n",
    "aa1  = r\"(?:[A-Za-z]\\.){2,}(?=\" + boundaryNotDot + \")\"\n",
    "aa2  = r\"[^A-Za-z](?:[A-Za-z]\\.){1,}[A-Za-z](?=\" + boundaryNotDot + \")\"\n",
    "standardAbbreviations = r\"\\b(?:[Mm]r|[Mm]rs|[Mm]s|[Dd]r|[Ss]r|[Jj]r|[Rr]ep|[Ss]en|[Ss]t)\\.\"\n",
    "arbitraryAbbrev = regex_or(aa1, aa2, standardAbbreviations)\n",
    "separators  = \"(?:--+|―|—|~|–|=)\"\n",
    "decorations = u\"(?:[♫♪]+|[★☆]+|[♥❤♡]+|[\\u2639-\\u263b]+|[\\ue001-\\uebbb]+)\"\n",
    "thingsThatSplitWords = r\"[^\\s\\.,?\\\"]\"\n",
    "embeddedApostrophe = thingsThatSplitWords+r\"+['’′]\" + thingsThatSplitWords + \"*\"\n",
    "normalEyes = \"[:=]\" # 8 and x are eyes but cause problems\n",
    "wink = \"[;]\"\n",
    "noseArea = \"(?:|-|[^a-zA-Z0-9 ])\" # doesn't get :'-(\n",
    "happyMouths = r\"[D\\)\\]\\}]+\"\n",
    "sadMouths = r\"[\\(\\[\\{]+\"\n",
    "tongue = \"[pPd3]+\"\n",
    "otherMouths = r\"(?:[oO]+|[/\\\\]+|[vV]+|[Ss]+|[|]+)\" # remove forward slash if http://'s aren't cleaned\n",
    "\n",
    "# mouth repetition examples:\n",
    "# @aliciakeys Put it in a love song :-))\n",
    "# @hellocalyclops =))=))=)) Oh well\n",
    "\n",
    "# myleott: try to be as case insensitive as possible, but still not perfect, e.g., o.O fails\n",
    "#bfLeft = u\"(♥|0|o|°|v|\\\\$|t|x|;|\\u0ca0|@|ʘ|•|・|◕|\\\\^|¬|\\\\*)\".encode('utf-8')\n",
    "bfLeft = u\"(♥|0|[oO]|°|[vV]|\\\\$|[tT]|[xX]|;|\\u0ca0|@|ʘ|•|・|◕|\\\\^|¬|\\\\*)\"\n",
    "bfCenter = r\"(?:[\\.]|[_-]+)\"\n",
    "bfRight = r\"\\2\"\n",
    "s3 = r\"(?:--['\\\"])\"\n",
    "s4 = r\"(?:<|&lt;|>|&gt;)[\\._-]+(?:<|&lt;|>|&gt;)\"\n",
    "s5 = \"(?:[.][_]+[.])\"\n",
    "# myleott: in Python the (?i) flag affects the whole expression\n",
    "#basicface = \"(?:(?i)\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "basicface = \"(?:\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "\n",
    "eeLeft = r\"[＼\\\\ƪԄ\\(（<>;ヽ\\-=~\\*]+\"\n",
    "eeRight= u\"[\\\\-=\\\\);'\\u0022<>ʃ）/／ノﾉ丿╯σっµ~\\\\*]+\"\n",
    "eeSymbol = r\"[^A-Za-z0-9\\s\\(\\)\\*:=-]\"\n",
    "eastEmote = eeLeft + \"(?:\"+basicface+\"|\" +eeSymbol+\")+\" + eeRight\n",
    "\n",
    "oOEmote = r\"(?:[oO]\" + bfCenter + r\"[oO])\"\n",
    "\n",
    "emoticon = regex_or(\n",
    "        # Standard version  :) :( :] :D :P\n",
    "        \"(?:>|&gt;)?\" + regex_or(normalEyes, wink) + regex_or(noseArea,\"[Oo]\") + regex_or(tongue+r\"(?=\\W|$|RT|rt|Rt)\", otherMouths+r\"(?=\\W|$|RT|rt|Rt)\", sadMouths, happyMouths),\n",
    "\n",
    "        # reversed version (: D:  use positive lookbehind to remove \"(word):\"\n",
    "        # because eyes on the right side is more ambiguous with the standard usage of : ;\n",
    "        regex_or(\"(?<=(?: ))\", \"(?<=(?:^))\") + regex_or(sadMouths,happyMouths,otherMouths) + noseArea + regex_or(normalEyes, wink) + \"(?:<|&lt;)?\",\n",
    "\n",
    "        #inspired by http://en.wikipedia.org/wiki/User:Scapler/emoticons#East_Asian_style\n",
    "        eastEmote.replace(\"2\", \"1\", 1), basicface,\n",
    "        # iOS 'emoji' characters (some smileys, some symbols) [\\ue001-\\uebbb]\n",
    "        # TODO should try a big precompiled lexicon from Wikipedia, Dan Ramage told me (BTO) he does this\n",
    "\n",
    "        # myleott: o.O and O.o are two of the biggest sources of differences\n",
    "        #          between this and the Java version. One little hack won't hurt...\n",
    "        oOEmote\n",
    ")\n",
    "\n",
    "Hearts = \"(?:<+/?3+)+\" #the other hearts are in decorations\n",
    "\n",
    "Arrows = regex_or(r\"(?:<*[-―—=]*>+|<+[-―—=]*>*)\", u\"[\\u2190-\\u21ff]+\")\n",
    "\n",
    "Hashtag = \"#[a-zA-Z0-9_]+\"\n",
    "AtMention = \"[@＠][a-zA-Z0-9_]+\"\n",
    "\n",
    "Bound = r\"(?:\\W|^|$)\"\n",
    "Email = regex_or(\"(?<=(?:\\W))\", \"(?<=(?:^))\") + r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}(?=\" +Bound+\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be tokenizing using these regexps as delimiters\n",
    "# Additionally, these things are \"protected\", meaning they shouldn't be further split themselves.\n",
    "Protected  = re.compile(\n",
    "    regex_or(\n",
    "        Hearts,\n",
    "        url,\n",
    "        Email,\n",
    "        timeLike,\n",
    "        monetary,\n",
    "        numberWithCommas,\n",
    "        numComb,\n",
    "        emoticon,\n",
    "        Arrows,\n",
    "        entity,\n",
    "        punctSeq,\n",
    "        arbitraryAbbrev,\n",
    "        separators,\n",
    "        decorations,\n",
    "        embeddedApostrophe,\n",
    "        Hashtag,\n",
    "        AtMention), re.UNICODE)\n",
    "\n",
    "# Edge punctuation\n",
    "# Want: 'foo' => ' foo '\n",
    "# While also:   don't => don't\n",
    "# the first is considered \"edge punctuation\".\n",
    "# the second is word-internal punctuation -- don't want to mess with it.\n",
    "# BTO (2011-06): the edgepunct system seems to be the #1 source of problems these days.\n",
    "# I remember it causing lots of trouble in the past as well.  Would be good to revisit or eliminate.\n",
    "\n",
    "# Note the 'smart quotes' (http://en.wikipedia.org/wiki/Smart_quotes)\n",
    "#edgePunctChars    = r\"'\\\"“”‘’«»{}\\(\\)\\[\\]\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunctChars    = u\"'\\\"“”‘’«»{}\\\\(\\\\)\\\\[\\\\]\\\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunct    = \"[\" + edgePunctChars + \"]\"\n",
    "notEdgePunct = \"[a-zA-Z0-9]\" # content characters\n",
    "offEdge = r\"(^|$|:|;|\\s|\\.|,)\"  # colon here gets \"(hello):\" ==> \"( hello ):\"\n",
    "EdgePunctLeft  = re.compile(offEdge + \"(\"+edgePunct+\"+)(\"+notEdgePunct+\")\", re.UNICODE)\n",
    "EdgePunctRight = re.compile(\"(\"+notEdgePunct+\")(\"+edgePunct+\"+)\" + offEdge, re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitEdgePunct(input):\n",
    "    input = EdgePunctLeft.sub(r\"\\1\\2 \\3\", input)\n",
    "    input = EdgePunctRight.sub(r\"\\1 \\2\\3\", input)\n",
    "    return input\n",
    "\n",
    "# The main work of tokenizing a tweet.\n",
    "def simpleTokenize(text):\n",
    "\n",
    "    # Do the no-brainers first\n",
    "    splitPunctText = splitEdgePunct(text)\n",
    "\n",
    "    textLength = len(splitPunctText)\n",
    "\n",
    "    # BTO: the logic here got quite convoluted via the Scala porting detour\n",
    "    # It would be good to switch back to a nice simple procedural style like in the Python version\n",
    "    # ... Scala is such a pain.  Never again.\n",
    "\n",
    "    # Find the matches for subsequences that should be protected,\n",
    "    # e.g. URLs, 1.0, U.N.K.L.E., 12:53\n",
    "    bads = []\n",
    "    badSpans = []\n",
    "    for match in Protected.finditer(splitPunctText):\n",
    "        # The spans of the \"bads\" should not be split.\n",
    "        if (match.start() != match.end()): #unnecessary?\n",
    "            bads.append( [splitPunctText[match.start():match.end()]] )\n",
    "            badSpans.append( (match.start(), match.end()) )\n",
    "\n",
    "    # Create a list of indices to create the \"goods\", which can be\n",
    "    # split. We are taking \"bad\" spans like\n",
    "    #     List((2,5), (8,10))\n",
    "    # to create\n",
    "    #     List(0, 2, 5, 8, 10, 12)\n",
    "    # where, e.g., \"12\" here would be the textLength\n",
    "    # has an even length and no indices are the same\n",
    "    indices = [0]\n",
    "    for (first, second) in badSpans:\n",
    "        indices.append(first)\n",
    "        indices.append(second)\n",
    "    indices.append(textLength)\n",
    "\n",
    "    # Group the indices and map them to their respective portion of the string\n",
    "    splitGoods = []\n",
    "    for i in range(0, len(indices), 2):\n",
    "        goodstr = splitPunctText[indices[i]:indices[i+1]]\n",
    "        splitstr = goodstr.strip().split(\" \")\n",
    "        splitGoods.append(splitstr)\n",
    "\n",
    "    #  Reinterpolate the 'good' and 'bad' Lists, ensuring that\n",
    "    #  additonal tokens from last good item get included\n",
    "    zippedStr = []\n",
    "    for i in range(len(bads)):\n",
    "        zippedStr = addAllnonempty(zippedStr, splitGoods[i])\n",
    "        zippedStr = addAllnonempty(zippedStr, bads[i])\n",
    "    zippedStr = addAllnonempty(zippedStr, splitGoods[len(bads)])\n",
    "\n",
    "    # BTO: our POS tagger wants \"ur\" and \"you're\" to both be one token.\n",
    "    # Uncomment to get \"you 're\"\n",
    "    #splitStr = []\n",
    "    #for tok in zippedStr:\n",
    "    #    splitStr.extend(splitToken(tok))\n",
    "    #zippedStr = splitStr\n",
    "\n",
    "    return zippedStr\n",
    "\n",
    "\n",
    "def addAllnonempty(master, smaller):\n",
    "    for s in smaller:\n",
    "        strim = s.strip()\n",
    "        if (len(strim) > 0):\n",
    "            master.append(strim)\n",
    "    return master\n",
    "\n",
    "# \"foo   bar \" => \"foo bar\"\n",
    "def squeezeWhitespace(input):\n",
    "    return Whitespace.sub(\" \", input).strip()\n",
    "\n",
    "# Final pass tokenization based on special patterns\n",
    "def splitToken(token):\n",
    "    m = Contractions.search(token)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [token]\n",
    "\n",
    "# Assume 'text' has no HTML escaping.\n",
    "def tokenize(text):\n",
    "    return simpleTokenize(squeezeWhitespace(text))\n",
    "\n",
    "# Twitter text comes HTML-escaped, so unescape it.\n",
    "# We also first unescape &amp;'s, in case the text has been buggily double-escaped.\n",
    "def normalizeTextForTagger(text):\n",
    "    assert sys.version_info[0] >= 3 and sys.version_info[1] > 3, 'Python version >3.3 required'\n",
    "    text = text.replace(\"&amp;\", \"&\")\n",
    "    text = html.unescape(text)\n",
    "    return text\n",
    "\n",
    "# This is intended for raw tweet text -- we do some HTML entity unescaping before running the tagger.\n",
    "#\n",
    "# This function normalizes the input text BEFORE calling the tokenizer.\n",
    "# So the tokens you get back may not exactly correspond to\n",
    "# substrings of the original text.\n",
    "def tokenizeRawTweetText(text):\n",
    "    tokens = tokenize(normalizeTextForTagger(text))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Wrapper for POS Tagger\n",
    "- The functions below call the runTagger.sh to get the POS tag predictions for the tokenized tweets. \n",
    "- runTagger.sh script should be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TAGGER_CMD = \"java -XX:ParallelGCThreads=2 -Xmx500m -jar ark-tweet-nlp-0.3.2.jar\"\n",
    "\n",
    "def _split_results(rows):\n",
    "    \"\"\"Parse the tab-delimited returned lines, modified from: https://github.com/brendano/ark-tweet-nlp/blob/master/scripts/show.py\"\"\"\n",
    "    for line in rows:\n",
    "        line = line.strip()  # remove '\\n'\n",
    "        if len(line) > 0:\n",
    "            if line.count('\\t') == 2:\n",
    "                parts = line.split('\\t')\n",
    "                tokens = parts[0]\n",
    "                tags = parts[1]\n",
    "                confidence = float(parts[2])\n",
    "                yield tokens, tags, confidence\n",
    "                \n",
    "                \n",
    "def _call_runtagger(tweets, run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Call runTagger.sh using a named input file\"\"\"\n",
    "\n",
    "    # remove carriage returns as they are tweet separators for the stdin\n",
    "    # interface\n",
    "    tweets_cleaned = [tw.replace('\\n', ' ') for tw in tweets]\n",
    "    message = \"\\n\".join(tweets_cleaned)\n",
    "\n",
    "    # force UTF-8 encoding (from internal unicode type) to avoid .communicate encoding error as per:\n",
    "    # http://stackoverflow.com/questions/3040101/python-encoding-for-pipe-communicate\n",
    "    message = message.encode('utf-8')\n",
    "\n",
    "    # build a list of args\n",
    "    args = shlex.split(run_tagger_cmd)\n",
    "    args.append('--output-format')\n",
    "    args.append('conll')\n",
    "    po = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    # old call - made a direct call to runTagger.sh (not Windows friendly)\n",
    "    #po = subprocess.Popen([run_tagger_cmd, '--output-format', 'conll'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    result = po.communicate(message)\n",
    "    # expect a tuple of 2 items like:\n",
    "    # ('hello\\t!\\t0.9858\\nthere\\tR\\t0.4168\\n\\n',\n",
    "    # 'Listening on stdin for input.  (-h for help)\\nDetected text input format\\nTokenized and tagged 1 tweets (2 tokens) in 7.5 seconds: 0.1 tweets/sec, 0.3 tokens/sec\\n')\n",
    "\n",
    "    pos_result = result[0].decode('utf-8').strip('\\n\\n')  # get first line, remove final double carriage return\n",
    "    pos_result = pos_result.split('\\n\\n')  # split messages by double carriage returns\n",
    "    pos_results = [pr.split('\\n') for pr in pos_result]  # split parts of message by each carriage return\n",
    "    return pos_results\n",
    "\n",
    "\n",
    "def runtagger_parse(tweets, run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Call runTagger.sh on a list of tweets, parse the result, return lists of tuples of (term, type, confidence)\"\"\"\n",
    "    pos_raw_results = _call_runtagger(tweets, run_tagger_cmd)\n",
    "    pos_result = []\n",
    "    for pos_raw_result in pos_raw_results:\n",
    "        pos_result.append([x for x in _split_results(pos_raw_result)])\n",
    "    return pos_result\n",
    "\n",
    "\n",
    "def check_script_is_present(run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Simple test to make sure we can see the script\"\"\"\n",
    "    success = False\n",
    "    try:\n",
    "        args = shlex.split(run_tagger_cmd)\n",
    "        args.append(\"--help\")\n",
    "        po = subprocess.Popen(args, stdout=subprocess.PIPE)\n",
    "        # old call - made a direct call to runTagger.sh (not Windows friendly)\n",
    "        #po = subprocess.Popen([run_tagger_cmd, '--help'], stdout=subprocess.PIPE)\n",
    "        while not po.poll():\n",
    "            lines = [l for l in po.stdout]\n",
    "        # we expected the first line of --help to look like the following:\n",
    "        assert \"RunTagger [options]\" in lines[0].decode('utf-8')\n",
    "        success = True\n",
    "    except OSError as err:\n",
    "        print(\"Caught an OSError, have you specified the correct path to runTagger.sh? We are using \\\"%s\\\". Exception: %r\" % (run_tagger_cmd, repr(err)))\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read tokenized tweets\n",
    "We will now load tweets that have the tokenized for POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tweets_tokenized.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1d3a79410d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tweets_tokenized.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtweets_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tweets_tokenized.txt'"
     ]
    }
   ],
   "source": [
    "file = open(\"tweets_tokenized.txt\", \"r\")\n",
    "tweets_tokenized = file.readlines()\n",
    "print(tweets_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply POS tagger\n",
    "The output of the POS tagger is a tuple containing token, predicted output tag, and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n",
      "[[]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ccc5b2ec7b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtokenized_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizeRawTweetText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mruntagger_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenized_tweet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#tokenized_tweet = ' '.join(simpleTokenize(line))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-0555fa736218>\u001b[0m in \u001b[0;36mruntagger_parse\u001b[0;34m(tweets, run_tagger_cmd)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mruntagger_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_tagger_cmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRUN_TAGGER_CMD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m\"\"\"Call runTagger.sh on a list of tweets, parse the result, return lists of tuples of (term, type, confidence)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mpos_raw_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_runtagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_tagger_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mpos_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos_raw_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_raw_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-0555fa736218>\u001b[0m in \u001b[0;36m_call_runtagger\u001b[0;34m(tweets, run_tagger_cmd)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--output-format'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conll'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mpo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# old call - made a direct call to runTagger.sh (not Windows friendly)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#po = subprocess.Popen([run_tagger_cmd, '--output-format', 'conll'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    727\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1316\u001b[0m                 \u001b[0merrpipe_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m                     \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m                     \u001b[0merrpipe_data\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrpipe_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raw_text = 'alcohol_tweets_4k.txt'\n",
    "oup_file = open(\"tweets_pos.txt\", \"w\") \n",
    "inp_file = open(raw_text)\n",
    "#oup_file = open(\"tweets_tokenized.txt\", \"w\") \n",
    "for line in inp_file:\n",
    "    tokenized_tweet = ' '.join(tokenizeRawTweetText(line))\n",
    "    out = runtagger_parse([tokenized_tweet])\n",
    "    print(out)\n",
    "    #tokenized_tweet = ' '.join(simpleTokenize(line))\n",
    "    #print('t: ' + tokenized_tweet + '\\n')\n",
    "    #print(line.strip()+'\\t'+tokenized_tweet+'\\t'+str(out[0]) + '\\n')\n",
    "    oup_file.write(line.strip()+'\\t'+tokenized_tweet+'\\t'+str(out[0]) + '\\n')\n",
    "inp_file.close()\n",
    "oup_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
