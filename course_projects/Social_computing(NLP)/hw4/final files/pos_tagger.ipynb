{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter POS Tagging \n",
    "The goal of this tutorial is to introduce a a Part-of-Speech (POS) tagger developed for tweets which was released as part of the [TweetNLP](https://www.ark.cs.cmu.edu/TweetNLP/) toolkit. The code is written in Java and the python wrapper for the tokenization is from [this](https://github.com/myleott/ark-twokenize-py) github repository. This tutorial has code from the [TweetNLP](https://github.com/brendano/ark-tweet-nlp/) github repository as well as the python wrapper from [this](https://github.com/ianozsvald/ark-tweet-nlp-python) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "- POS tagging involves identifying part-of-speech of tokens in a given text. This can be viewed as a task of labeling the sentence w_1, w_2, ....., w_n with pos tags, one for each word: t_1, t_2, ...., t_n.\n",
    "- The 8 common parts of speech for english language are:\n",
    "  1. Noun\n",
    "  2. Verb\n",
    "  3. Pronoun\n",
    "  4. Preposition\n",
    "  5. Adverb\n",
    "  6. Conjuction\n",
    "  7. Participle\n",
    "  8. Article  \n",
    "- Twitter data is different from standard language data in that there are tokens such as #, @, emoticons, URLs, etc. So the tagset for twitter needs to incorporate the tags for these new tokens. The tags that are used to annotate tweets are as follows:\n",
    "\n",
    "<img src=\"pos_tags.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments\n",
    "- This tutorial covers how to accomplish the task of POS tagging for twitter data based on this paper: https://aclanthology.org/P11-2008.pdf\n",
    "- The nature of twitter data poses challenges in using standard POS taggers. The paper develops the above tagset for twitter to include tags for words that are not commonly encountered in language outside of twitter. \n",
    "- Around 1,800 tweets were manually annotated with corresponding pos tags.\n",
    "- Conditional Random Fields (CRFs) were used with features specific to twitter POS tagging. The features for the CRF are below (see paper for more details):\n",
    "  - Twitter orthography - these features are rules that detect @, #, and URls.\n",
    "  - Names - these features check for names from a dictionary of compiled tokens which are frequently capitalized.\n",
    "  - Traditional Tag Dictionary - these are features for all tags that occur in PTB.\n",
    "  - Distributional Similarity - these features are constructed from the successor and predecessor probabilities for the 10,000 most common terms.\n",
    "  - Phonetic normalization - words are normalized to ignore alternate spellings of words using the Metaphone algorithm; e.x.{thangs, thanks, thanksss, thanx, thinks, thnx} are mapped to 0NKS.\n",
    "- 1827 tweets that are annotated are divided into training set of 1000 tweets, dev set of 327 tweets, and test set of 500 tweets. The results of the tagger incorporating the above features are compared with the standard Stanford Tagger and using the above feature set for twitter data reduces error by about 25%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "- You will need to download the POS tagger from https://code.google.com/archive/p/ark-tweet-nlp/downloads\n",
    "- This requires Java 6. https://www.oracle.com/java/technologies/java-platform.html\n",
    "- Place this ipython notebook that has python wrappers inside the ark-tweet-nlp-0.3.2 folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "try:\n",
    "    from html.parser import HTMLParser\n",
    "except ImportError:\n",
    "    from HTMLParser import HTMLParser\n",
    "\n",
    "try:\n",
    "    import html\n",
    "except ImportError:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Contractions = re.compile(u\"(?i)(\\w+)(n['‚Äô‚Ä≤]t|['‚Äô‚Ä≤]ve|['‚Äô‚Ä≤]ll|['‚Äô‚Ä≤]d|['‚Äô‚Ä≤]re|['‚Äô‚Ä≤]s|['‚Äô‚Ä≤]m)$\", re.UNICODE)\n",
    "Whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
    "punctChars = r\"['\\\"‚Äú‚Äù‚Äò‚Äô.?!‚Ä¶,:;]\"\n",
    "punctSeq   = r\"['\\\"‚Äú‚Äù‚Äò‚Äô]+|[.?!,‚Ä¶]+|[:;]+\"\n",
    "entity     = r\"&(?:amp|lt|gt|quot);\" # see more here https://www.w3schools.com/html/html_entities.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_or(*items):\n",
    "    return '(?:' + '|'.join(items) + ')'\n",
    "\n",
    "urlStart1  = r\"(?:https?://|\\bwww\\.)\"\n",
    "commonTLDs = r\"(?:com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xxx)\"\n",
    "ccTLDs = r\"(?:ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|\" + \\\n",
    "r\"bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|\" + \\\n",
    "r\"er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|\" + \\\n",
    "r\"hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|\" + \\\n",
    "r\"lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|\" + \\\n",
    "r\"nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|\" + \\\n",
    "r\"sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|\" + \\\n",
    "r\"va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)\"\t#TODO: remove obscure country domains?\n",
    "urlStart2  = r\"\\b(?:[A-Za-z\\d-])+(?:\\.[A-Za-z0-9]+){0,3}\\.\" + regex_or(commonTLDs, ccTLDs) + r\"(?:\\.\"+ccTLDs+r\")?(?=\\W|$)\"\n",
    "urlBody    = r\"(?:[^\\.\\s<>][^\\s<>]*?)?\"\n",
    "urlExtraCrapBeforeEnd = regex_or(punctChars, entity) + \"+?\"\n",
    "urlEnd     = r\"(?:\\.\\.+|[<>]|\\s|$)\"\n",
    "url        = regex_or(urlStart1, urlStart2) + urlBody + \"(?=(?:\"+urlExtraCrapBeforeEnd+\")?\"+urlEnd+\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "monetary = r\"\\$([0-9]+)?\\.?([0-9]+)?\"\n",
    "timeLike   = r\"\\d+(?::\\d+){1,2}\"\n",
    "numberWithCommas = r\"(?:(?<!\\d)\\d{1,3},)+?\\d{3}\" + r\"(?=(?:[^,\\d]|$))\"\n",
    "numComb = u\"[\\u0024\\u058f\\u060b\\u09f2\\u09f3\\u09fb\\u0af1\\u0bf9\\u0e3f\\u17db\\ua838\\ufdfc\\ufe69\\uff04\\uffe0\\uffe1\\uffe5\\uffe6\\u00a2-\\u00a5\\u20a0-\\u20b9]?\"\n",
    "boundaryNotDot = regex_or(\"$\", r\"\\s\", r\"[‚Äú\\\"?!,:;]\", entity)\n",
    "aa1  = r\"(?:[A-Za-z]\\.){2,}(?=\" + boundaryNotDot + \")\"\n",
    "aa2  = r\"[^A-Za-z](?:[A-Za-z]\\.){1,}[A-Za-z](?=\" + boundaryNotDot + \")\"\n",
    "standardAbbreviations = r\"\\b(?:[Mm]r|[Mm]rs|[Mm]s|[Dd]r|[Ss]r|[Jj]r|[Rr]ep|[Ss]en|[Ss]t)\\.\"\n",
    "arbitraryAbbrev = regex_or(aa1, aa2, standardAbbreviations)\n",
    "separators  = \"(?:--+|‚Äï|‚Äî|~|‚Äì|=)\"\n",
    "decorations = u\"(?:[‚ô´‚ô™]+|[‚òÖ‚òÜ]+|[‚ô•‚ù§‚ô°]+|[\\u2639-\\u263b]+|[\\ue001-\\uebbb]+)\"\n",
    "thingsThatSplitWords = r\"[^\\s\\.,?\\\"]\"\n",
    "embeddedApostrophe = thingsThatSplitWords+r\"+['‚Äô‚Ä≤]\" + thingsThatSplitWords + \"*\"\n",
    "normalEyes = \"[:=]\" # 8 and x are eyes but cause problems\n",
    "wink = \"[;]\"\n",
    "noseArea = \"(?:|-|[^a-zA-Z0-9 ])\" # doesn't get :'-(\n",
    "happyMouths = r\"[D\\)\\]\\}]+\"\n",
    "sadMouths = r\"[\\(\\[\\{]+\"\n",
    "tongue = \"[pPd3]+\"\n",
    "otherMouths = r\"(?:[oO]+|[/\\\\]+|[vV]+|[Ss]+|[|]+)\" # remove forward slash if http://'s aren't cleaned\n",
    "\n",
    "# mouth repetition examples:\n",
    "# @aliciakeys Put it in a love song :-))\n",
    "# @hellocalyclops =))=))=)) Oh well\n",
    "\n",
    "# myleott: try to be as case insensitive as possible, but still not perfect, e.g., o.O fails\n",
    "#bfLeft = u\"(‚ô•|0|o|¬∞|v|\\\\$|t|x|;|\\u0ca0|@| ò|‚Ä¢|„Éª|‚óï|\\\\^|¬¨|\\\\*)\".encode('utf-8')\n",
    "bfLeft = u\"(‚ô•|0|[oO]|¬∞|[vV]|\\\\$|[tT]|[xX]|;|\\u0ca0|@| ò|‚Ä¢|„Éª|‚óï|\\\\^|¬¨|\\\\*)\"\n",
    "bfCenter = r\"(?:[\\.]|[_-]+)\"\n",
    "bfRight = r\"\\2\"\n",
    "s3 = r\"(?:--['\\\"])\"\n",
    "s4 = r\"(?:<|&lt;|>|&gt;)[\\._-]+(?:<|&lt;|>|&gt;)\"\n",
    "s5 = \"(?:[.][_]+[.])\"\n",
    "# myleott: in Python the (?i) flag affects the whole expression\n",
    "#basicface = \"(?:(?i)\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "basicface = \"(?:\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "\n",
    "eeLeft = r\"[Ôºº\\\\∆™‘Ñ\\(Ôºà<>;„ÉΩ\\-=~\\*]+\"\n",
    "eeRight= u\"[\\\\-=\\\\);'\\u0022<> ÉÔºâ/Ôºè„ÉéÔæâ‰∏ø‚ïØœÉ„Å£¬µ~\\\\*]+\"\n",
    "eeSymbol = r\"[^A-Za-z0-9\\s\\(\\)\\*:=-]\"\n",
    "eastEmote = eeLeft + \"(?:\"+basicface+\"|\" +eeSymbol+\")+\" + eeRight\n",
    "\n",
    "oOEmote = r\"(?:[oO]\" + bfCenter + r\"[oO])\"\n",
    "\n",
    "emoticon = regex_or(\n",
    "        # Standard version  :) :( :] :D :P\n",
    "        \"(?:>|&gt;)?\" + regex_or(normalEyes, wink) + regex_or(noseArea,\"[Oo]\") + regex_or(tongue+r\"(?=\\W|$|RT|rt|Rt)\", otherMouths+r\"(?=\\W|$|RT|rt|Rt)\", sadMouths, happyMouths),\n",
    "\n",
    "        # reversed version (: D:  use positive lookbehind to remove \"(word):\"\n",
    "        # because eyes on the right side is more ambiguous with the standard usage of : ;\n",
    "        regex_or(\"(?<=(?: ))\", \"(?<=(?:^))\") + regex_or(sadMouths,happyMouths,otherMouths) + noseArea + regex_or(normalEyes, wink) + \"(?:<|&lt;)?\",\n",
    "\n",
    "        #inspired by http://en.wikipedia.org/wiki/User:Scapler/emoticons#East_Asian_style\n",
    "        eastEmote.replace(\"2\", \"1\", 1), basicface,\n",
    "        # iOS 'emoji' characters (some smileys, some symbols) [\\ue001-\\uebbb]\n",
    "        # TODO should try a big precompiled lexicon from Wikipedia, Dan Ramage told me (BTO) he does this\n",
    "\n",
    "        # myleott: o.O and O.o are two of the biggest sources of differences\n",
    "        #          between this and the Java version. One little hack won't hurt...\n",
    "        oOEmote\n",
    ")\n",
    "\n",
    "Hearts = \"(?:<+/?3+)+\" #the other hearts are in decorations\n",
    "\n",
    "Arrows = regex_or(r\"(?:<*[-‚Äï‚Äî=]*>+|<+[-‚Äï‚Äî=]*>*)\", u\"[\\u2190-\\u21ff]+\")\n",
    "\n",
    "Hashtag = \"#[a-zA-Z0-9_]+\"\n",
    "AtMention = \"[@Ôº†][a-zA-Z0-9_]+\"\n",
    "\n",
    "Bound = r\"(?:\\W|^|$)\"\n",
    "Email = regex_or(\"(?<=(?:\\W))\", \"(?<=(?:^))\") + r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}(?=\" +Bound+\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be tokenizing using these regexps as delimiters\n",
    "# Additionally, these things are \"protected\", meaning they shouldn't be further split themselves.\n",
    "Protected  = re.compile(\n",
    "    regex_or(\n",
    "        Hearts,\n",
    "        url,\n",
    "        Email,\n",
    "        timeLike,\n",
    "        monetary,\n",
    "        numberWithCommas,\n",
    "        numComb,\n",
    "        emoticon,\n",
    "        Arrows,\n",
    "        entity,\n",
    "        punctSeq,\n",
    "        arbitraryAbbrev,\n",
    "        separators,\n",
    "        decorations,\n",
    "        embeddedApostrophe,\n",
    "        Hashtag,\n",
    "        AtMention), re.UNICODE)\n",
    "\n",
    "# Edge punctuation\n",
    "# Want: 'foo' => ' foo '\n",
    "# While also:   don't => don't\n",
    "# the first is considered \"edge punctuation\".\n",
    "# the second is word-internal punctuation -- don't want to mess with it.\n",
    "# BTO (2011-06): the edgepunct system seems to be the #1 source of problems these days.\n",
    "# I remember it causing lots of trouble in the past as well.  Would be good to revisit or eliminate.\n",
    "\n",
    "# Note the 'smart quotes' (http://en.wikipedia.org/wiki/Smart_quotes)\n",
    "#edgePunctChars    = r\"'\\\"‚Äú‚Äù‚Äò‚Äô¬´¬ª{}\\(\\)\\[\\]\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunctChars    = u\"'\\\"‚Äú‚Äù‚Äò‚Äô¬´¬ª{}\\\\(\\\\)\\\\[\\\\]\\\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunct    = \"[\" + edgePunctChars + \"]\"\n",
    "notEdgePunct = \"[a-zA-Z0-9]\" # content characters\n",
    "offEdge = r\"(^|$|:|;|\\s|\\.|,)\"  # colon here gets \"(hello):\" ==> \"( hello ):\"\n",
    "EdgePunctLeft  = re.compile(offEdge + \"(\"+edgePunct+\"+)(\"+notEdgePunct+\")\", re.UNICODE)\n",
    "EdgePunctRight = re.compile(\"(\"+notEdgePunct+\")(\"+edgePunct+\"+)\" + offEdge, re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitEdgePunct(input):\n",
    "    input = EdgePunctLeft.sub(r\"\\1\\2 \\3\", input)\n",
    "    input = EdgePunctRight.sub(r\"\\1 \\2\\3\", input)\n",
    "    return input\n",
    "\n",
    "# The main work of tokenizing a tweet.\n",
    "def simpleTokenize(text):\n",
    "\n",
    "    # Do the no-brainers first\n",
    "    splitPunctText = splitEdgePunct(text)\n",
    "\n",
    "    textLength = len(splitPunctText)\n",
    "\n",
    "    # BTO: the logic here got quite convoluted via the Scala porting detour\n",
    "    # It would be good to switch back to a nice simple procedural style like in the Python version\n",
    "    # ... Scala is such a pain.  Never again.\n",
    "\n",
    "    # Find the matches for subsequences that should be protected,\n",
    "    # e.g. URLs, 1.0, U.N.K.L.E., 12:53\n",
    "    bads = []\n",
    "    badSpans = []\n",
    "    for match in Protected.finditer(splitPunctText):\n",
    "        # The spans of the \"bads\" should not be split.\n",
    "        if (match.start() != match.end()): #unnecessary?\n",
    "            bads.append( [splitPunctText[match.start():match.end()]] )\n",
    "            badSpans.append( (match.start(), match.end()) )\n",
    "\n",
    "    # Create a list of indices to create the \"goods\", which can be\n",
    "    # split. We are taking \"bad\" spans like\n",
    "    #     List((2,5), (8,10))\n",
    "    # to create\n",
    "    #     List(0, 2, 5, 8, 10, 12)\n",
    "    # where, e.g., \"12\" here would be the textLength\n",
    "    # has an even length and no indices are the same\n",
    "    indices = [0]\n",
    "    for (first, second) in badSpans:\n",
    "        indices.append(first)\n",
    "        indices.append(second)\n",
    "    indices.append(textLength)\n",
    "\n",
    "    # Group the indices and map them to their respective portion of the string\n",
    "    splitGoods = []\n",
    "    for i in range(0, len(indices), 2):\n",
    "        goodstr = splitPunctText[indices[i]:indices[i+1]]\n",
    "        splitstr = goodstr.strip().split(\" \")\n",
    "        splitGoods.append(splitstr)\n",
    "\n",
    "    #  Reinterpolate the 'good' and 'bad' Lists, ensuring that\n",
    "    #  additonal tokens from last good item get included\n",
    "    zippedStr = []\n",
    "    for i in range(len(bads)):\n",
    "        zippedStr = addAllnonempty(zippedStr, splitGoods[i])\n",
    "        zippedStr = addAllnonempty(zippedStr, bads[i])\n",
    "    zippedStr = addAllnonempty(zippedStr, splitGoods[len(bads)])\n",
    "\n",
    "    # BTO: our POS tagger wants \"ur\" and \"you're\" to both be one token.\n",
    "    # Uncomment to get \"you 're\"\n",
    "    #splitStr = []\n",
    "    #for tok in zippedStr:\n",
    "    #    splitStr.extend(splitToken(tok))\n",
    "    #zippedStr = splitStr\n",
    "\n",
    "    return zippedStr\n",
    "\n",
    "\n",
    "def addAllnonempty(master, smaller):\n",
    "    for s in smaller:\n",
    "        strim = s.strip()\n",
    "        if (len(strim) > 0):\n",
    "            master.append(strim)\n",
    "    return master\n",
    "\n",
    "# \"foo   bar \" => \"foo bar\"\n",
    "def squeezeWhitespace(input):\n",
    "    return Whitespace.sub(\" \", input).strip()\n",
    "\n",
    "# Final pass tokenization based on special patterns\n",
    "def splitToken(token):\n",
    "    m = Contractions.search(token)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [token]\n",
    "\n",
    "# Assume 'text' has no HTML escaping.\n",
    "def tokenize(text):\n",
    "    return simpleTokenize(squeezeWhitespace(text))\n",
    "\n",
    "# Twitter text comes HTML-escaped, so unescape it.\n",
    "# We also first unescape &amp;'s, in case the text has been buggily double-escaped.\n",
    "def normalizeTextForTagger(text):\n",
    "    assert sys.version_info[0] >= 3 and sys.version_info[1] > 3, 'Python version >3.3 required'\n",
    "    text = text.replace(\"&amp;\", \"&\")\n",
    "    text = html.unescape(text)\n",
    "    return text\n",
    "\n",
    "# This is intended for raw tweet text -- we do some HTML entity unescaping before running the tagger.\n",
    "#\n",
    "# This function normalizes the input text BEFORE calling the tokenizer.\n",
    "# So the tokens you get back may not exactly correspond to\n",
    "# substrings of the original text.\n",
    "def tokenizeRawTweetText(text):\n",
    "    tokens = tokenize(normalizeTextForTagger(text))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Wrapper for POS Tagger\n",
    "- The functions below call the runTagger.sh to get the POS tag predictions for the tokenized tweets. \n",
    "- runTagger.sh script should be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TAGGER_CMD = \"java -XX:ParallelGCThreads=2 -Xmx500m -jar ark-tweet-nlp-0.3.2.jar\"\n",
    "\n",
    "def _split_results(rows):\n",
    "    \"\"\"Parse the tab-delimited returned lines, modified from: https://github.com/brendano/ark-tweet-nlp/blob/master/scripts/show.py\"\"\"\n",
    "    for line in rows:\n",
    "        line = line.strip()  # remove '\\n'\n",
    "        if len(line) > 0:\n",
    "            if line.count('\\t') == 2:\n",
    "                parts = line.split('\\t')\n",
    "                tokens = parts[0]\n",
    "                tags = parts[1]\n",
    "                confidence = float(parts[2])\n",
    "                yield tokens, tags, confidence\n",
    "                \n",
    "                \n",
    "def _call_runtagger(tweets, run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Call runTagger.sh using a named input file\"\"\"\n",
    "\n",
    "    # remove carriage returns as they are tweet separators for the stdin\n",
    "    # interface\n",
    "    tweets_cleaned = [tw.replace('\\n', ' ') for tw in tweets]\n",
    "    message = \"\\n\".join(tweets_cleaned)\n",
    "\n",
    "    # force UTF-8 encoding (from internal unicode type) to avoid .communicate encoding error as per:\n",
    "    # http://stackoverflow.com/questions/3040101/python-encoding-for-pipe-communicate\n",
    "    message = message.encode('utf-8')\n",
    "\n",
    "    # build a list of args\n",
    "    args = shlex.split(run_tagger_cmd)\n",
    "    args.append('--output-format')\n",
    "    args.append('conll')\n",
    "    po = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    # old call - made a direct call to runTagger.sh (not Windows friendly)\n",
    "    #po = subprocess.Popen([run_tagger_cmd, '--output-format', 'conll'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    result = po.communicate(message)\n",
    "    # expect a tuple of 2 items like:\n",
    "    # ('hello\\t!\\t0.9858\\nthere\\tR\\t0.4168\\n\\n',\n",
    "    # 'Listening on stdin for input.  (-h for help)\\nDetected text input format\\nTokenized and tagged 1 tweets (2 tokens) in 7.5 seconds: 0.1 tweets/sec, 0.3 tokens/sec\\n')\n",
    "\n",
    "    pos_result = result[0].decode('utf-8').strip('\\n\\n')  # get first line, remove final double carriage return\n",
    "    pos_result = pos_result.split('\\n\\n')  # split messages by double carriage returns\n",
    "    pos_results = [pr.split('\\n') for pr in pos_result]  # split parts of message by each carriage return\n",
    "    return pos_results\n",
    "\n",
    "\n",
    "def runtagger_parse(tweets, run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Call runTagger.sh on a list of tweets, parse the result, return lists of tuples of (term, type, confidence)\"\"\"\n",
    "    pos_raw_results = _call_runtagger(tweets, run_tagger_cmd)\n",
    "    pos_result = []\n",
    "    for pos_raw_result in pos_raw_results:\n",
    "        pos_result.append([x for x in _split_results(pos_raw_result)])\n",
    "    return pos_result\n",
    "\n",
    "\n",
    "def check_script_is_present(run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Simple test to make sure we can see the script\"\"\"\n",
    "    success = False\n",
    "    try:\n",
    "        args = shlex.split(run_tagger_cmd)\n",
    "        args.append(\"--help\")\n",
    "        po = subprocess.Popen(args, stdout=subprocess.PIPE)\n",
    "        # old call - made a direct call to runTagger.sh (not Windows friendly)\n",
    "        #po = subprocess.Popen([run_tagger_cmd, '--help'], stdout=subprocess.PIPE)\n",
    "        while not po.poll():\n",
    "            lines = [l for l in po.stdout]\n",
    "        # we expected the first line of --help to look like the following:\n",
    "        assert \"RunTagger [options]\" in lines[0].decode('utf-8')\n",
    "        success = True\n",
    "    except OSError as err:\n",
    "        print(\"Caught an OSError, have you specified the correct path to runTagger.sh? We are using \\\"%s\\\". Exception: %r\" % (run_tagger_cmd, repr(err)))\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read tokenized tweets\n",
    "We will now load tweets that have the tokenized for POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I won't win a single game I bet on!! Got Mr. Cliff Lee, if he loses its on me U.S.A!\\n\", 'RT @eye_e: this poster-print costs $12.40 , which is 40% of the normal price! http://tl.gd/6meogh\\n', 'I ‚ù§ Biebs & want to hang out with him!!\\n', '@thecamion I like monkeys, but I still hate COSTCO parking lots.. oO o.O #COSTCO 2:15 PM\\n', 'Texas Rangers are in the World Series! Go Rangers!!!!!!!!! :> <3 ‚ô•‚ù§‚ô° http://fb.me/D2LsXBJx\\n']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"tweets_tokenized.txt\", \"r\")\n",
    "tweets_tokenized = file.readlines()\n",
    "print(tweets_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply POS tagger\n",
    "The output of the POS tagger is a tuple containing token, predicted output tag, and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', ':', 'to', 'in', 'i', 'a', '@', 'you', '!', '#job', 'at', 'and', 'for', '#hiring', '-', 'my', '‚Ä¶', 'this', 'of', ')', '?', 'is', '\"', 'me', 'on', \"i'm\", '(', '#jobs', 'be', 'it', 'with', '...', 'so', 'that', 'just', 'our', '#careerarc', 'de', '&', 'can', 'up', 'like', 'latest', 'no', 'have', 'out', 'was', 'but', 'see', 'want', 'your', 'love', 'all', 'great', 'are', '(@', 'here', 'not', 'we', 'get', \"we're\", 'if', '‚Äú', 'la', 'about', 'work', 'when', 'opening', 'day', 'time', 'click', \"don't\", '‚Äù', \"it's\", '|', 'what', 'from', 'new', 'do', 'que', 'go', 'by', 'one', 'good', 'ca', 'today', '#retail', '!!', '‚ù§', 'anyone', 'or', '#tweetmyjobs', 'now', 'lol', '..', 'know', '#hospitality', 'got', 'fit', 'en', 'el', 'y', 'apply', 'recommend', 'happy', 'back', 'how', 'tx', 'Ô∏è', 'they', '#veterans', 'u', '!!!', \"'\", \"you're\", 'some', 'could', 'will', 'he', '2', '->', 'night', 'as', \"can't\", 'an', 'people', 'team', 'oh', '#nursing', 'need', 'more', 'mi', 'who', 'really', 'life', 'too', 'why', 'going', 'fl', 'home', 'down', 'shit', 'best', 'she', 'health', 'üòÇ', 'much', '#sales', 'her', 'am', 'us', 'right', 'had', 'been', 'sales', 'ny', '3', 'wind', 'might', 'off', 'near', '#healthcare', 'last', 'come', 'view', 'manager', 'still', 'never', 'make', 'alert', 'read', 'birthday', 'only', 'think', '1', 'check', 'humidity', 'los', 'has', 'beach', 'tonight', 'park', 'city', 'there', 'join', 'would', 'always', 'ya', '/', 'even', 'thanks', 'first', 'fuck', '$', 'these', 'im', '....', 'interested', \"that's\", 'over', 'way', 'school', 'man', 'se', 'his', 'san', 'them', 'details', 'drinking', '‚Äî', 'take', 'photo', 'thank', 'rn', 'morning', 'con', 'center', 'es', 'care', '0.0', 'did', '4', 'look', 'ever', 'las', 'little', 'store', '5', '0', 'say', 'posted', 'wanna', 'then', 'after', 'feel', 'girl', 'te', 'better', 'por', 'miss', 'getting', 'stop', 'game', 'il', 'tomorrow', 'temperature', 'than', 'gonna', 'because', 'retail', 'house', 'un', 'someone', 'being', 'york', 'ass', 'pa', 'w/', '*', 'where', \"i'll\", 'st', 'nurse', 'hate', 'yo', 'him', 'real', 'its', 'df', 'para', 'well', 'va', 'del', 'friends', 'ga', \"i've\", 'associate', 'lo', 'year', '#it', 'wait', 'beautiful', 'week', 'sleep', 'days', 'next', 'pressure', 'please', 'big', 'ma', 'bad', 'part', 'show', 'ok', 'were', 'world', '[', 'made', 'nj', 'before', ']', 'again', 'fun', 'place', 'god', 'should', 'co', 'baby', 'every', 'let', 'rain', 'ready', 'rt', 'professional', 'assistant', 'long', 'service', 'n', 'hope', 'two', 'al', 'thing', 'üòç', 'their', 'follow', 'fucking', '6', 'something', 'cloudy', 'nc', 'nigga', 'yes', '#transportation', 'portland', 'old', '#trndnl', 'into', 'high', 'said', \"ain't\", 'bitch', 'everyone', 'medical', ':)', '‚ò∫', 'live', 'üòÇüòÇ', 'watch', 'things', '2015', '29', 'another', '!!!!', 'nice', \"didn't\", 'same', 'done', 'amazing', 'pretty', 'una', 'weekend', 'm√©xico', 'tell', 'mo', 'state', 'favorite', 'guys', 'family', 'keep', 'wish', '10', 'bar', 'damn', 'full', 'hard', 'friend', 'looking', '7', 'give', 'weather', 'lmao', 'play', 'california', 'gotta', 'doing', 'si', 'start', 'call', 'free', 'mexico', 'food', 'music', 'everything', ';', 'club', 'try', 'wa', 'w', 'üòí', '8', 'other', 'mom', 'ave', 'girls', 'hours', '#customerservice', 'already', 'very', 'most', '#businessmgmt', 'party', 'lake', 'find', 'üòÇüòÇüòÇ', 'shift', 'cause', 'registered', 'north', 'street', 'talk', 'falling', 'yeah', 'nothing', 'tu', 'starbucks', 'pm', 'having', 'hot', 'mph', 'airport', 'senior', 'rising', 'summer', 'person', 'trying', 'group', 'video', 'around', 'many', 'hair', 'coming', 'restaurant', 'cool', 'az', 'half', 'hey', 'put', 'years', 'omg', 'finally', 'üò≠', 'friday', 'como', 'specialist', 'watching', 'any', 'job', 'makes', '30', '9', 'while', 'west', 'class', 'south', 'sure', 'through', 'myself', 'accident', 'üò©', 'services', 'twitter', 'üòä', 'md', 'supervisor', 'does', 'awesome', 'left', 'international', 'phone', 'break', 'room', 'sc', 'white', 'boy', 'such', 'chicago', 'ur', 'e', 'cute', \"let's\", '.‚Ä¶', 'road', '#photo', 'car', 'technician', 'until', 'crazy', 'help', 'black', 'police', 'away', 'since', 'those', \"he's\", '#accounting', 'guy', 'mean', 'tn', 'bc', 's', 'bed', 'stay', 'actually', 'lot', 'sunday', 'tho', 'o', 'fair', 'university', 'thought', 'end', 'wi', 'st.', 'rd', 'money', 'point', 'spring', 'christmas', \"doesn't\", 'barometer', 'hotel', 'times', '#houston', 'making', 'head', 'heart', 'representative', 'hospital', 'sonic', 'saturday', 'visibility', 'wow', 'hit', 'bro', '??', 'drive', 'haha', 'though', \"what's\", '#manufacturing', 'fire', 'drive-in', 'went', 'face', 'mn', 'tweet', 'cvs', 'perfect', 'guess', 'pero', 'af', 'soon', '#traffic', 'federal', 'hi', 'open', 'early', 'niggas', '#clerical', 'ne', 'gone', 'took', 'support', 'talking', 'looks', 'run', 'via', 'dont', 'current', 'texas', 'eat', 'okay', 'sweet', 'le', '#pdx911', 'anything', 'feeling', 'block', 'win', 'event', 'excited', 'top', 'sorry', 'yall', 'hell', 'found', 'mad', 'late', 'coffee', 'text', '#engineering', 'general', 'funny', 'dinner', '.....', \"there's\", ',‚Ä¶', 'company', 'distrito', 'beer', 'miami', 'whole', 'vegas', 'name', 'season', 'sunset', 'believe', 'leave', 'true', 'red', 'college', 'playing', 'working', 'engineer', 'own', 'tired', 'island', 'mind', \"she's\", 'needed', 'business', 'literally', 'waiting', 'trends', \"won't\", 'may', 'song', 'without', 'florida', 'm√°s', 'pizza', 'yet', 'bout', 'came', 'son', 'ct', 'taking', 'kids', 'dude', 'picture', '¬∞f', 'avenue', 'üòò', 'also', 'national', 'fall', 'b', 'todo', 'almost', 'houston', 'partly', 'angeles', 'customer', 'd', 'dad', 'remember', 'seen', '#realestate', 'santa', 'lil', 'quiero', 'monday', 'east', 'line', 'wrong', 'boys', 'lunch', 'sometimes', 'closed', 'traffic', 'lost', 'aerotek', 'r', 'both', 'told', 'analyst', 'x', 'rock', 'together', \"y'all\", 'post', 'super', '15', 'country', 'maybe', 'hoy', 'use', 'church', '#newyork', '12', 'few', 'enough', 'washington', 'office', 'probably', 'dog', 'meet', '#marketing', 'wtf', ':(', 'saw', 'gets', 'ft', 'da', 'side', 'travel', 'driver', 'idk', 'exit', 'change', 'esta', 'hour', 'department', 'living', '10mi', 'ask', 'bae', \"isn't\", 'movie', 'field', 'art', 'ii', 'bring', 'francisco', 'else', \"haven't\", 'walk', 'robert', '#nyc', 'cold', 'downtown', 'üëå', 'd√≠a', 'üòè', 'forever', 'sad', '?!', 'county', '20', '#orlando', 'c', 'needs', 'water', 'gym', 'town', '#orlpol', 'says', 'pharmacy', 'yesterday', 'blue', 'parts', 'smh', 'ciudad', \"i'd\", 'dead', 'vs', 'glad', 'mas', 'üòé', 'üòÅ', 'cuando', '#skilledtrade', 'case', 'pic', 'nv', \"today's\", 'cafe', 'date', 'su', 'marketing', 'turn', 'sunrise', '#chicago', 'steady', 'welcome', 'inc', 'üíØ', 'between', '!‚Ä¶', 'ky', '#nblnathaliavoto', 'chicken', 'ice', 'set', 'station', 'stadium', 'ago', 'body', 'vida', 'must', 'sick', 'proud', 'cook', 'puerto', '#labor', 'once', 'wanted', 'central', 'dc', 'thinking', 'swear', 'hear', 'trip', 'feels', 'consultant', 'buy', 'market', 'gracias', 'river', 'forecast', 'started', 'special', 'brother', 'casa', 'far', 'mejor', 'üòî', 'grill', 'crew', 'till', 'listen', 'goes', 'running', 'üôå', 'light', 'supplemental', 'wants', 'used', 'bay', '#atlanta', 'beauty', 'square', 'üíï', 'veterans', 'green', 'rico', 'member', 'anymore', 'bitches', '#finance', 'easy', '+', 'resort', 'grand', 'atlanta', '#sonic', 'snow', 'outside', 'thursday', 'yourself', 'seriously', 'shop', '!!!!!', '#tbt', 'saying', 'worth', 'ride', 'construction', \"they're\", 'dr', 'tour', 'em', 'breakfast', 'üëÄ', 'area', 'wake', 'request', 'seeing', 'ks', 'slowly', '???', 'f', 'stuff', 'sister', 'nobody', 'delivery', 'affairs', 'nb', 'fam', 'üòÖ', 'move', 'chill', 'truck', 'kinda', 'plaza', 'austin', 'young', 'ver', '=', 'close', 'üòÇüòÇüòÇüòÇ', 'called', 'which', 'drink', 'everybody', 'q', '#physician', 'comes', 'shot', 'front', 'update', 'mostly', '11', 'somebody', 'hay', 'least', 'tuesday', 'second', 'bien', 'trending', 'üò¥', 'financial', 'stupid', 'story', 'project', 'topic', 'beat', 'minutes', 'boston', '1st', 'eyes', 'enjoy', 'valley', \"wasn't\", 'üòå', 'bruh', 'mins', 'lead', 'fitness', 'missing', 'reason', 'toronto', 'order', \"couldn't\", 'oracle', 'nyc', '#repost', 'mis', 'sb', 'mine', 'brooklyn', 'tan', 'solo', 'american', 'three', '#love', 'hollywood', 'coordinator', 'honestly', 'past', 'system', 'staff', 'bridge', 'different', 'bank', 'nights', '#losangeles', \"macy's\", 'kind', 'diego', '‚Äì', 'ima', 'america', 'cant', 'die', 'single', 'este', 'moment', 'sin', 'forget', 'museum', 'social', 'understand', 'fine', 'wednesday', 'weeks', 'amo', 'blocked', 'ale', 'director', 'blvd', 'garden', 'delay', 'dream', 'straight', 'barista', 'dance', '#miami', 'siempre', 'sun', 'dallas', '#kellyjobs', 'm', '#kca', 'amor', 'hall', 'media', 'nah', 'auto', 'kid', 'ipa', 'hurt', 'alone', 'hill', 'star', 'snapchat', 'l', 'starting', 'bye', 'each', 'pick', 'games', 'fight', 'lounge', '~', 'community', '#parttime', 'k', 'worst', 'course', 'tryna', 'during', 'month', '>', '#dallas', '#art', 'pay', \"who's\", 'ppl', 'visit', 'cut', 'kill', 'weird', 'lane', 'accounting', 'ha', '#sanfrancisco', '#makeup', 'positions', '---', 'selfie', 'words', 'hello', 'ser', 'wit', 'nos', ':-)', 'air', 'ugly', 'news', \"wouldn't\", 'cuz', 'puebla', 'fan', 'heard', 'memorial', 'server', 'ar', 'available', 'congrats', 'wear', 'definitely', 'send', 'sea', 'months', 'later', 'account', 'happen', 'seasonal', 'fucked', 'mood', 'nuevo', 'porque', 'eating', '0mph', 'corporation', 'mountain', 'lady', '//', 'centro', 'level', 'tf', 'future', 'training', '24', 'üò≥', 'bored', 'couple', 'rest', 'book', '#education', 'leader', 'woman', '‚Ä¢', '#menu', 'knew', 'john', 'fav', 'word', 'cream', 'ahora', 'inside', 'iphone', 'jersey', 'ia', 'final', 'orlando', 'drunk', 'management', 'muy', 'local', 'king', 'babe', 'üé∂', 'mall', '16', 'hahaha', 'ball', 'walking', 'feliz', 'fast', 'learn', 'sports', 'lets', 'ladies', 'pool', '#seattle', 'trust', 'everyday', 'met', 't', 'carolina', 'pictures', 'opened', 'domingo', 'catch', 'under', 'missed', 'july', 'tengo', 'usa', 'happened', 'advantage', 'fort', 'matter', 'number', 'clinical', 'tweets', 'women', '50', 'hills', \"you'll\", '14', 'ugh', 'fresh', 'blessed', 'monterrey', 'sign', 'festival', '#banking', 'bit', 'üòï', 'dixie', 'todos', 'key', 'idea', 'smile', 'eb', 'canada', 'üòâ', '4th', 'building', 'top20', '#kellyservices', 'public', 'jesus', '#columbus', '#', 'wine', 'cry', '100', 'wonder', '#austin', 'united', '--', 'nada', 'lmfao', 'cuauht√©moc', 'ni', 'üòã', 'quick', 'link', 'wb', 'hoes', 'per', '18', '#boston', 'driving', '#automotive', 'trucks', 'concert', '13', 'march', '2nd', 'temp', 'luck', 'district', 'trainee', 'gave', 'studio', 'knows', 'intern', '#phoenix', 'tech', 'act', 'loved', 'either', 'tv', 'winn', 'sw', 'woke', 'finished', 'seattle', '25', 'enjoying', 'dumb', 'crying', 'power', 'orange', 'holy', 'eso', 'parents', 'sounds', 'hungry', 'bread', 'forgot', 'lord', 'hand', 'wonderful', 'finance', 'üíÅ', 'ut', 'behind', 'giving', 'physician', 'vancouver', 'estoy', 'cheese', 'experience', 'philadelphia', 'deal', 'fans', 'lose', 'able', 'soy', '):', 'dress', 'football', 'camp', 'test', '#lasvegas', 'üòë', 'asked', 'as√≠', 'strong', 'practice', 'asleep', 'round', 'kitchen', 'sound', 'dios', 'less', 'tbh', 'listening', 'dm', 'technology', 'means', '#internship', 'gorgeous', 'med', 'sucks', 'brewing', 'thats', 'middle', 'problem', 'les', 'loves', 'spend', 'wearing', 'shut', 'ms', '#denver', 'dick', 'falls', 'yea', 'loving', 'kelly', 'theatre', '„Éª„Éª„Éª', 'aqu√≠', 'wild', '21', 'chocolate', 'goodnight', 'chance', 'mex', ';)', 'maintenance', 'stand', 'creek', 'lucky', '‚ô•', 'golf', 'üôè', 'hold', 'album', 'plan', 'bday', 'üòê', 'hoe', 'against', 'üî•', 'developer', 'solutions', 'relationship', 'fuckin', 'tried', '#veteranjob', 'prom', 'fact', 'miguel', 'software', 'main', 'shopping', 'd√≠as', 'ti', 'small', 'men', 'village', 'boyfriend', 'üòà', 'üíÄ', 'tree', 'workout', 'forward', 'wedding', 'program', 'takes', 'georgia', 'double', 'nw', 'color', '6mph', 'goals', 'bet', 'spot', 'following', 'ones', 'g', 'smoke', 'mt', 'saint', 'nap', 'meeting', 'hidalgo', 'miles', 'queen', 'tendencia', 'cat', 'tickets', 'id', 'low', 'gay', 'evening', 'gust', 'virginia', 'fat', 'disney', 'bus', '#hr', 'type', 'serious', \"aren't\", 'network', 'therapist', 'whatever', 'four', 'holiday', \"you've\", '......', 'broke', 'sitting', 'hang', '#sandiego', 'mucho', '5mph', 'eres', '#carolinawx', 'short', 'ocean', 'share', 'jobs', 'v', 'ju√°rez', 'drop', 'pics', 'pt', 'brown', 'coach', 'feelings', 'sleeping', 'instead', 'save', '#sanantonio', 'suck', 'j', 'annoying', 'hands', 'ohio', 'systems', 'winter', '3mph', '#nowplaying', 'development', 'fake', '17', 'lovely', 'sunny', 'guadalajara', 'reports', 'jajaja', 'dreams', 'hasta', 'p', 'supply', 'colorado', 'realize', 'nm', 'killed', 'mother', 'human', 'throw', 'exactly', '<3', 'leaving', '3rd', 'sit', 'app', 'death', 'boo', '#toronto', 'afternoon', 'data', 'roll', 'soul', 'sky', 'par', 'deep', '#facilitiesmgmt', 'sex', 'voy', 'map', 'laugh', 'style', 'basketball', 'huge', 'absolutely', 'quality', 'using', 'band', 'lie', 'squad', 'slow', 'shoot', 'üòçüòç', 'cosmetics', 'noche', '\\\\', 'advisor', 'üòú', 'door', 'insurance', 'pass', 'yours', 'sushi', 'self', \"here's\", 'staffing', ':/', 'scanned', 'counter', 'campus', 'baseball', 'report', 'hanging', 't-t', 'ese', 'rather', '#opd', 'santo', 'michigan', 'trail', '#supplychain', 'pain', 'magic', 'won', 'stage', 'hola', 'minute', 'h', '7mph', 'dark', 'vamos', 'arizona', 'control', '#ocso', 'dollar', 'starts', 'buena', 'studios', 'apple', 'clean', 'moving', 'happens', 'mama', '#summer', '#listing', 'merry', 'benito', 'springs', \"o'reilly\", 'voice', 'shirt', 'clerk', 'sale', 'question', 'b/w', 'üíô', 'pls', 'pussy', 'operations', 'download', 'halloween', 'buenos', 'semana', 'pub', 'production', '19', 'inn', 'celebrating', 'design', 'üò¢', 'tacos', '#indianapolis', 'ma√±ana', 'factory', 'track', 'imma', 'desde', 'cake', 'spent', 'aint', 'hashtag', 'vehicle', 'radio', 'coyoac√°n', 'casino', 'hace', 'plus', 'kentucky', 'hiring', 'graffiti', 'research', 'sexy', 'killing', 'space', 'burger', 'üòçüòçüòç', 'add', 'played', 'til', 'eye', 'üò™', 'snap', 'nunca', 'gift', 'baltimore', 'tea', '#nofilter', 'scared', 'homie', 'spa', 'fin', 'veracruz', 'chris', 'denver', 'caf√©', 'hilton', 'finish', 'shower', 'golden', '8mph', 's/o', 'thru', '2016', 'security', 'instagram', '#springfield', 'na', 'box', 'shoes', 'patient', 'truth', 'yay', 'fr', 'jack', 'juan', '#portland', '#washington', 'nursing', 'union', ',...', 'alright', 'awake', 'child', 'personal', 'dia', 'gold', 'gresham', 'safe', 'thankful', 'üò≠üò≠', 'farm', 'become', 'official', 'playa', 'nh', 'flight', 'felt', 'online', '#family', 'mr.', '#pharmaceutical', 'important', 'shows', 'piece', 'tattoo', 'bought', 'mercy', 'foto', 'due', 'internacional', 'physical', 'train', 'diem', 'calm', '40', 'totally', 'panera', 'regional', 'vez', 'louis', 'lots', 'works', 'daddy', 'caught', 'arts', 'hurts', 'vitamin', 'list', 'ontario', 'soliant', 'shoppe', 'others', 'speedway', 'calling', 'pr', 'cannot', 'asf', 'arena', 'ill', 'james', 'appreciate', 'movies', 'nl', 'along', 'step', 'taco', 'entry', 'dirty', 'lights', 'hacer', 'series', 'executive', 'voto', 'looked', 'ime', 'finna', '22', '#earthquake', 'pues', 'across', 'quien', \"tomorrow's\", 'respect', 'parking', 'history', 'sent', 'bothdir', 'pull', 'currently', 'trash', '#music', 'vote', 'qu√©', 'extra', 'pier', 'fish', 'hwy', 'algo', 'court', '#legal', 'justin', '#vote5sos', 'üòÇüòÇüòÇüòÇüòÇ', 'le√≥n', 'wife', 'mundo', 'buen', 'kick', 'fell', 'antonio', 'üòû', 'biggest', 'reading', 'aeropuerto', 'repostapp', 'tiene', 'watched', 'mouth', 'dr.', 'taken', 'utc', 'hella', 'lit', 'coast', 'van', 'ex', 'outta', 'favor', 'charlotte', 'ri', 'parque', 'energy', \"we'll\", 'attention', 'episode', 'stuck', 'stopped', 'floor', 'bless', 'cup', 'wings', 'netflix', 'taste', 'supposed', '#oklahomacity', '!!!!!!', 'jal', 'putting', 'emergency', 'telling', 'üëç', 'carhop', 'at&t', 'vista', 'carhop/skating', 'ion', 'phoenix', 'warm', '‚ô°', 'delicious', 'üò≠üò≠üò≠', 'dying', 'nd', 'entire', '<', 'restaurants', 'joke', 'tap', 'cleveland', 'kansas', 'seems', 'fue', 'celebrate', 'grow', 'clear', 'busy', '#columbia', 'jalisco', 'shooting', 'üíú', 'session', 'adventure', 'cousin', 'race', 'vacation', 'happening', '0in', 'truly', 'somewhere', 'moon', 'mexican', 'facebook', 'brought', 'oklahoma', 'fashion', 'columbia', 'sr.', 'esto', 'peace', 'dat', 'ah', 'officially', 'üôà', 'gas', 'pissed', 'laughing', 'paper', 'nacional', 'answer', 'üòÑ', '#sunset', '#kansascity', 'usgs', 'deserve', 'empire', '#charlotte', 'bueno', 'dope', 'operator', 'sus', 'tiempo', 'feet', '#selfie', 'tus', 'iii', 'august', 'warehouse', 'six', 'shots', 'delays', 'chef', 'zoo', '#travel', 'plans', 'student', '#jacksonville', 'worry', '#richmond', 'bowl', 'treat', 'hopefully', '#nashville', 'soccer', 'accountant', '#california', 'edt', 'oomf', 'wall', 'states', 'pue', 'ups', 'brand', 'five', 'salon', 'grade', 'heat', 'earth', 'goal', 'cleaning', 'gente', 'turned', 'gran', 'cashier', 'orleans', 'direction', 'fly', 'üíñ', '#friends', 'teacher', 'mission', 'june', 'nashville', 'technical', 'healthcare', 'tractor', 'lab', 'ir', 'express', '#vscocam', '#weather', 'performance', 'detroit', 'dogs', 'shout', 'estar', '23', 'ab', 'providence', 'üéâ', 'cada', 'changed', 'anybody', 'sense', 'oregon', 'alive', 'üíã', 'wv', 'land', 'product', 'page', 'hop', 'sr', 'especially', 'kiss', '#insurance', 'heaven', 'ily', '27', 'buffalo', 'table', 'alguien', 'internet', 'accountemps', 'students', 'michael', 'girlfriend', 'greatest', 'commercial', 'liberty', 'worse', 'speak', '100%', 'cosas', 'baker', 'certified', 'theater', 'dos', 'gon', 'booty', '‚†Ä', '#claytonnc_wx', '‚ú®', '‚úåÔ∏è', 'success', 'madison', 'm√©rida', 'pop', 'irish', 'gives', 'dear', 'est√°', '||', 'asking', 'universidad', 'brothers', '#coupon', 'beyond', 'law', 'natural', 'ate', 'homework', '#beach', \"we've\", 'dj', 'heavy', 'donde', 'songs', 'buddy', 'artist', 'tl', 'bill', '#philadelphia', 'paul', 'cam', 'estas', '#food', 'clothes', 'mechanic', 'agree', 'wash', '9mph', 'blood', 'million', '#projectmgmt', 'nails', '#usa', 'lives', 'min', 'sam', 'showers', '‚òÄÔ∏è', 'applications', 'buyer', 'force', 'manhattan', 'harbor', 'earlier', 'southern', 'seem', 'ran', 'officeteam', 'bio', 'smart', '26', 'interesting', 'esa', 'universal', 'window', 'winning', 'km', 'bros', 'oak', 'size', 'hawaii', 'legal', 'saludos', 'complete', 'columbus', 'unit', 'uno', 'match', 'forest', 'stock', 'üí™', '28', 'possible', 'message', 'tampa', '#1', 'bbq', 'random', 'calls', 'route', 'drake', 'worker', '@mariobautista_', 'daily', 'opportunity', 'weak', 'daughter', 'drinks', 'jose', 'glass', 'ihop', 'library', '#foodporn', 'rose', 'illinois', 'april', 'george', 'touch', 'trade', 'indiana', 'charles', \"shouldn't\", 'lincoln', 'congratulations', 'cover', 'puedo', 'president', 'cin√©polis', 'alarm', 'waking', 'internship', 'taylor', 'player', 'bag', 'likes', '#mexico', 'quote', 'contact', '#cincinnati', 'merchandiser', 'sleepy', 'pink', \"bj's\", 'üì∑', 'blast', 'rip', 'mike', 'action', 'viernes', 'moved', 'classic', 'makeup', 'simple', 'row', 'learning', 'broken', 'card', '#interpreter', 'convention', 'photos', '!?', 'sooo', 'monica', 'brunch', 'üíò', 'singing', 'pennsylvania', 'officer', 'estado', 'aureus', 'listing', 'stops', 'happiness', 'palm', 'thoughts', 'avi', 'paid', '#empleo', 'spending', 'menu', 'jaja', 'yard', '#us_test', 'incredible', 'meant', 'longer', 'stick', 'mess', 'android', 'boulevard', 'centre', 'icu', 'showing', 'corner', '#realtor', 'wars', 'animal', 'bell', 'lugar', 'port', 'estate', 'cheesecake', 'location', 'memories', '#omaha', 'tag', 'loud', 'bike', 'dancing', '#tampa', 'thinks', 'aka', '#endomondo', 'legs', 'client', 'bullshit', '#sanjose', '#cintasjobs', 'cdt', 'freaking', 'bear', 'thx', 'upset', 'honor', 'died', 'butt', 'brewery', 'mal', 'bomb', 'conference', 'pray', '#la', 'promise', 'dry', '#christmas', 'terminal', 'qc', '@nashgrier', 'buenas', 'bacon', ':3', 'keeps', 'xd', 'je', 'jackson', 'bp', 'a√±os', '#burlington', 'david', 'creative', '#regions', 'choice', 'amigos', 'books', 'speech', 'heading', 'maryland', 'rh', 'lowkey', 'heights', 'cloud', 'trend', 'info', 'anniversary', 'jajajaja', 'quite', 'administrative', 'study', 'decided', 'lately', 'bound', 'dormir', 'goodbye', 'luv', 'residential', 'videos', 'plz', 'posici√≥n', 'poor', 'pittsburgh', 'calgary', 'total', 'goin', 'loss', '#cleveland', 'guatemala', '#dialysis', 'soooo', 'global', 'sing', '#brooklyn', 'spirit', 'citi', 'sugar', 'base', 'designer', 'age', 'basically', 'meat', 'pumpkin', 'boss', 'cleared', 'pants', 'jealous', 'problems', 'record', 'horas', 'paradise', 'stout', 'french', 'villahermosa', 'trap', 'er', 'throwback', 'added', 'major', '#nowhiring', 'october', 'apparently', 'administrator', 'rich', 'children', 'war', 'yoga', 'mark', 'sat', 'tower', 'dp', 'ranch', 'fantastic', 'classes', '5th', 'disneyland', 'original', 'cancer', 'everywhere', 'bakery', 'grove', 'massachusetts', 'drivers', 'tavern', 'pedro', 'father', 'angel', '00', 'üíó', 'hood', 'licensed', '????', 'clinic', 'fix', 'players', 'idc', 'storm', 'third', 'üá∫üá∏', 'weed', 'retweet', 'completely', 'accounts', 'places', 'maurices', 'des', 'board', 'born', 'iowa', 'followers', 'knowing', 'üòõ', 'cr', 'credit', \"vera's\", 'smell', 'juice', '#bilingual', '#minneapolis', 'write', 'ella', 'stone', 'bottle', 'puede', ':d', 'surprise', 'honey', 'thunderstorms', 'otra', 'pro', '‚Äô', '#rhfajobs', 'resources', 'hahahaha', 'canyon', 'yup', 'üëè', 'smith', 'princess', 'stores', 'folks', 'tanto', 'trouble', 'stars', 'lay', 'normal', 'email', 'mix', 'soo', 'gusta', 'babies', 'youtube', 'sisters', 'familia', 'yep', 'imagine', 'era', 'incident', '#louisville', 'count', 'ticket', 'hmu', '#empleos', \"lowe's\", '#allentown', 'joe', 'tlalpan', '‚úä', 'web', 'female', 'terrible', 'handle', 'salad', 'nope', 'nadie', '#security', '#milwaukee', 'ak', 'including', '#blessed', 'lanes', 'gettin', 'tax', 'sauce', 'learned', 'personas', '**', '#fun', 'lazy', 'seconds', 'momma', 'begin', 'don', 'üò§', 'growing', 'boca', 'skin', 'salt', 'letting', 'cst', 'cross', 'zayn', '45', 'rep', 'famous', 'league', 'none', '@justinbieber', 'tweeting', 'et', 'mb', 'hermosa', 'realized', 'fried', '#kcamexico', 'spanish', 'beef', '60', '75', 'nose', 'asap', '#sacramento', 'ways', 'despu√©s', 'um', '.,', 'milk', 'av', 'annual', 'site', 'model', 'choose', 'alaska', 'secret', 'quit', 'married', '#diversity', '#stlouis', 'healthy', 'expect', 'royal', 'sport', 'breaking', 'hoping', 'acting', 'interview', 'jordan', 'gardens', '10mph', '#officeteam', 'sur', 'texting', 'ig', 'except', '6th', 'mile', '99', 'yuc', 'keeping', 'target', 'gate', 'minneapolis', 'apartment', 'tough', 'steak', 'leg', 'boring', 'graduate', 'ring', 'stomach', 'pacific', 'üò´', 'grande', '#fitness', '24hrrn', 'mac', 'unless', 'beds', 'walked', 'contract', 'flying', 'staying', 'headed', 'kingdom', 'twin', '#restaurantjobs', 'tennessee', 'hora', 'sunshine', 'com', 'bestfriend', '#marchmadness', 'ocupando', 'period', 'boat', 'host', 'santiago', 'performing', 'moms', 'luis', 'grandma', '#latergram', 'üôä', 'scary', 'december', 'km/h', 'aw', '#baltimore', 'jam', 'ahead', 'baths', 'priority', 'eve', 'warning', 'huh', 'deli', 'confused', 'd.f.', 'machine', 'üòù', 'cheers', 'teams', 'anyway', '#aurora', 'hat', 'fb', 'cookie', 'pancakes', 'dressbarn', 'pack', 'richmond', 'pie', 'guest', 'wishing', 'nature', 'wins', '#paloalto', 'üòì', 'mini', 'jk', 'weight', 'press', '93%', 'principal', 'shake', '#regionsbank', 'chillin', 'üçÄ', 'dropped', '#construction', 'üòÇüò≠', 'losing', 'riding', 'passed', '90%', 'lee', 'quer√©taro', '#accountemps', 'freight', 'apartments', 'turning', 'tarde', 'metro', 'tears', 'awkward', 'silver', 'planning', 'üòª', 'jump', 'carmen', 'queens', 'mount', 'challenge', 'obama', 'candy', 'cruz', 'faith', 'sis', 'connection', 'bringing', 'planet', 'epic', 'blow', 'process', 'bracket', 'liked', '#generalscience', '#beautiful', 'virtual', 'appeared', 'positive', 'dayton', 'academy', '#memphis', \"disney's\", '‚úå', 'lpn', 'christian', 'english', 'pale', '#nettempsjobs', 'mentor', 'private', 'tournament', 'females', 'shoulder', 'creo', 'puppy', 'methodist', '#vancouver', 'horrible', 'fe', 'plane', 'canc√∫n', 'kevin', '#endorphins', 'conversation', 'helping', 'points', \"love's\", '.......', 'depot', 'gun', 'film', 'dental', 'selfies', 'jus', 'partner', 'shoutout', '#fashion', 'figure', 'meal', 'cars', 'harry', 'mm', 'luke', 'picked', 'cookies', 'twice', 'ast', 'bunch', 'fwy', 'fear', '@camerondallas', 'fighting', '#happy', 'bayada', \"mcdonald's\", \"tonight's\", 'a√±o', 'bright', 'wisconsin', 'worked', 'acabo', 'judge', 'shitty', '31', 'sidewalk', 'fox', 'ridge', '#raleigh', 'persona', 'coraz√≥n', 'cna', 'focus', '#halloween', 'fries', '‚Äò', \"it'll\", 'circle', 'digital', 'career', 'martinique', 'mf', '#nature', 'fiesta', 'joy', 'hilarious', 'struggle', 'vip', 'dan', 'fool', 'üíû', 'practitioner', 'matt', 'scott', 'diesel', 'vibes', 'baptist', 'nordstrom', 'üíî', \"children's\", '#vegas', 'trabajo', 'zapopan', 'language', 'lying', 'stress', 'talent', 'pride', 'surgery', 'science', 'grind', 'milwaukee', 'tt', 'oakland', 'acaba', 'lookin', 'chipotle', 'instructor', 'verdad', \"where's\", 'grateful', 'uncle', 'release', 'occupational', 'gallery', 'du', 'questions', 'tener', 'sharing', 'paso', 'reality', 'aqui', '))', 'smoking', 'indianapolis', 'ay', '#pittsburgh', 'corporate', 'often', 'tambi√©n', 'celebration', 'neighborhood', 'beginning', 'üò∑', 'ford', 'madness', 'butter', 'didnt', 'broadway', 's√≥lo', 'estadio', 'alex', 'difference', 'villa', 'cutest', 'camera', 'mention', 'above', '#sxsw', 'üåö', 'degrees', 'ground', 'wet', 'rice', 'cdl', 'noches', 'waste', 'seafood', 'hermann', 'solid', 'schedule', 'sell', 'üò°', 'ive', 'aye', 'mill', 'veces', 'holding', 'crush', '70', 'chick', 'üòÉ', 'palace', 'prn', 'bathroom', 'omfg', 'gang', '#vacation', 'large', \"you'd\", 'kidding', 'direct', 'chat', 'education', 'hitting', 'üåû', '#sundayfunday', 'situation', 'llc', 'followed', 'martin', 'vine', 'finding', 'antes', '10%', 'return', 'hearing', 'safety', 'bigger', 'events', 'collection', 'nasty', 'cheer', 'convertirse', '2014', 'wide', '#florida', 'mobile', 'duke', 'fest', '#detroit', '#intel', 'marina', 'vt', 'paint', 'comer', 'note', 'tienes', 't√∫', 'historic', '12mph', 'seven', 'usually', '#wcw', 'therapy', 'picking', 'sandwich', 'lame', 'mary', 'owner', 'known', '#tulsa', 'math', 'sacramento', 'acapulco', 'buying', 'tyco', 'information', 'baylor', 'doe', 'price', 'dice', 'build', 'reply', 'mixtape', 'parkway', 'disabled', 'arlington', 'zone', 'technologist', 'sold', '99%', '#cfgjobs', 'wayne', 'amigo', 'christ', 'bff', 'obsessed', 'jacksonville', 'screen', 'pour', 'labor', 'tom', 'excuse', 'throwing', '#starwars', 'newark', '#tucson', 'present', 'jones', 'doubt', 'donuts', 'wanting', '35', 'menos', '7th', 'enjoyed', 'flex', 'üêß', 'crap', 'hole', 'chase', 'cali', 'code', 'channel', 'üò±', 'momento', 'bottom', 'foot', 'est', 'grown', 'stories', '8th', 'dang', 'mar', 'alabama', '#arlington', 'given', 'battle', 'rap', 'hurry', 'hits', 'hhgregg', '#hawaii', 'republic', 'toda', \"patrick's\", 'iv', 'writing', '#coffee', 'begins', 'scene', 'cats', 'intersection', 'fresenius', 'cole', 'alcohol', '√†', 'ten', '(:', 'whenever', 'profile', 'qr', 'oil', 'parade', 'nightclub', 'continue', 'uv', 'tattoos', 'beats', 'grab', 'checking', 'otro', 'louisville', 'contigo', 'johnson', '#beermenus', 'desk', 'asi', 'pkwy', 'interpreters', 'sue√±o', 'bird', 'u.s.', '#greenville', 'memphis', 'adorable', '#contratar', 'blonde', 'bath', 'medicine', 'italian', 'garza', 'google', 'sir', 'foods', 'issues', 'regret', 'talked', 'üòÄ', 'surprised', 'lebron', 'chilling', \"father's\", 'horse', 'üíÉ', 'ud', 'dis', 'üî•üî•üî•', 'guard', 'branch', 'flow', 'gotten', 'stfu', 'finals', 'chief', 'receiving', 'bonus', 'apart', 'streets', '>>>', '#albuquerque', 'poco', 'barely', '{', 'nick', '#np', 'surg', 'estamos', 'whats', 'brain', 'burn', 'bieber', 'rude', 'nevada', 'mismo', '80', 'cards', 'grass', 'bf', 'youre', 'seat', 'wood', 'prep', 'arrived', 'yummy', 'feed', 'explain', 'position', 'üò©üò©', 'obviously', 'ended', 'ed', 'score', 'muchas', 'notice', 'attitude', 'mgr', 'hockey', '200', 'reasons', 'husband', 'castle', 'di', 'shore', 'max', 'surgical', 'friendship', 'slept', 'risk', '87%', 'held', 'eh', 'legit', 'named', 'successful', 'lemme', 'gto', 'siento', 'sand', 'decision', 'older', 'colors', 'blog', 'madre', 'ordering', 'turns', 'laying', 'property', 'completed', 'sa', 'sum', 'crash', 'dudes', 'falta', 'grilled', 'trump', 'selling', 'enterprises', 'aww', 'neck', 'haven', 'raleigh', 'peak', 'montreal', 'agent', 'championship', 'bella', 'cares', 'ending', 'üëë', 'yum', 'scattered', 'philly', 'amen', 'sd', 'plays', 'pulled', '#instagood', 'bagels', '90', 'moments', 'worldwide', 'rolling', 'linda', 'lies', 'midnight', 'üòòüòò', 'catching', 'compass', 'bonito', 'worried', 'uh', 'ben', '#retotelehit', 'victoria', 'cash', '#vsco', 'flood', 'üò£', 'rocks', 'moves', 'rocking', '#foodie', 'master', 'teach', 'steve', 'non', 'rio', 'anaheim', 'spectrum', 'hr', 'push', 'engineering', 'excelente', 'drugs', 'hip', 'baro', 'junior', '@shawnmendes', 'ryan', 'pet', 'cherry', '#quake', 'ln', 'newport', 'fabulous', 'wondering', 'himself', \"women's\", '#honolulu', 'ends', 'holidays', 'walmart', 'tabasco', 'temporary', 'woods', '21st', 'speaking', 'swim', 'üòÜ', 'crack', 'rise', 'espero', 'guanajuato', 'turnt', 'tab', 'honest', 'cuenta', 'practical', 'lunes', 'version', '#charleston', 'standing', 'chinese', 'texts', 'drama', '#lovefashionletstalk', 'pre', 'actual', 'ty', 'todas', 'sending', '!!!!!!!', 'charge', 'closer', 'blues', 'atlantic', '#rochester', 'tha', 'publicar', 'facultad', 'subway', 'nuestro', 'cumplea√±os', 'td', 'amount', 'bi-lo', 'attack', '#elpaso', 'transportation', 'talks', 'i‚Äôm', 'quiere', 'filled', 'featuring', 'it‚Äôs', 'cincinnati', 'computer', 'dias', 'crowd', 'valle', 'ganas', '#soundcloud', '√©', 'sean', 'craving', 'cancun', 'graduation', 'certain', 'advice', 'sol', 'carranza', 'freedom', 'military', 'mejores', 'dropping', 'brings', 'flags', 'massage', 'curry', 'mayor', 'hike', 'logistics', 'british', 'sabes', 'suites', '#sunrise', 'former', 'amiga', 'mil', 'bronx', 'extra√±o', 'afraid', '94%', '500', 'balls', 'egg', 'talented', 'pas', '50%', 'themselves', 'dawg', 'pair', \"should've\", '59', 'kendrick', 'casi', 'lax', 'plant', '#beer', 'rey', 'zona', 'youth', 'shorts', 'nation', 's√≠', 'hpa', 'tire', 'den', 'üíõ', 'threw', 'mia', '>>', 'venustiano', 'aurora', '#style', 'charleston', 'praying', 'injury', 'napa', 'alley', 'slide', 'üòÇüòÇüòÇüòÇüòÇüòÇ', 'blessing', 's√°bado', 'tells', 'package', 'üíö', 'leaves', 'summit', 'dame', 'xalapa', 'cc', 'ŸÖŸÜ', 'entre', 'cintas', 'giant', 'doin', 'minnesota', 'pasa', 'anyways', 'visiting', 'slp', 'clearly', \"hasn't\", 'ridiculous', 'insane', 'mr', 'hbd', 'changing', '#montreal', 'rainy', 'schools', '#socialmedia', 'üòá', 'blame', 'delete', 'lips', '#saltlakecity', 'atl', 'ho', 'venice', 'custom', 'feelin', 'teeth', '#mcm', 'darkness', 'western', 'speed', 'languageline', 'travelling', 'rough', '32', 'kennedy', 'select', 'harder', 'flowers', 'bud', '√≥n', 'kim', 'website', 'winner', 'üëØ', 'shine', 'sf', 'bob', 'kept', 'goat', 'relax', '65', 'based', 'characters', 'mental', 'sse', 'evil', 'anywhere', 'dominican', 'elementary', 'aids', 'review', 'bb', 'grace', 'lonely', 'üëä', 'opinion', '#neworleans', 'full-time', 'grey', '#claytonnc', 'capital', 'blunt', 'pleasure', '3d', 'excellent', 'adventures', 'changes', 'citizens', 'leon', 'http://t.co/aczacnvx3d', 'decide', 'journey', 'inch', 'records', 'kings', '}', 'serve', 'üò≠üò≠üò≠üò≠', 'porter', 'delta', 'showed', 'presents', 'arms', 'shall', 'annoyed', 'doctor', '#madison', 'japanese', 'headache', '#lexington', 'btw', '#photography', 'escuela', 'comment', 'issue', 'ktp', 'semester', 'tyler', '#sanjuan', 'childish', 'fruit', 'bistro', 'soft', 'savage', 'http://t.co/pa268hmek6', 'pure', 'couch', 'hashtags', '#fortworth', 'asshole', 'yellow', '#writing', 'quiet', 'outfit', 'roast', 'bang', 'mountains', 'bouta', '#fall', 'est√°n', 'naucalpan', 'whoever', 'cooking', 'cape', 'regular', 'beverly', 'members', 'continues', 'fave', 'jamaica', 'twittering', 'honolulu', 'lines', 'wrap', 'tendencias', 'wy', 'thomas', 'oaxaca', 'trees', 'tight', 'quarter', 'aunque', 'names', 'hype', 'faces', 'anytime', 'üòçüòçüòçüòç', 'ew', 'nba', 'civil', \"would've\", 'goodmorning', 'multnomah', 'hrs', '18th', 'common', 'legend', 'ando', 'views', 'clouds', 'barrel', 'gf', 'amc', 'happiest', 'title', 'felicidades', 'wassup', 'urban', 'walker', '#2015', 'bourbon', '#scottsdale', 'fourth', 'vas', 'suppose', 'einstein', '#texas', 'runs', 'diamond', 'cbre', 'nueva', 'wont', 'eggs', 'fireworks', 'kellymitchell', 'gunna', 'üò¨', '#fortmyers', 'morelos', '#roadtrip', 'signed', 'within', 'jay', 'indian', 'fw', '#canada', 'saved', 'lakes', 'sigue', 'dominicana', 'adult', 'chest', '#manhattan', 'hug', 'frank', 'severe', 'ohiohealth', 'dam', 'architect', 'entertainment', 'pan', '#fairfax', 'furniture', '!!‚Ä¶', 'everytime', 'empty', 'print', 'holder', 'upon', 'handsome', 'awful', '86', 'fault', 'duck', '#ihop', 'lmaoo', 'posting', 'companies', '?!?', '96%', 'bs', 'repair', 'cove', 'marriott', '55', 'frozen', 'angry', 'award', '#party', 'bull', \"everyone's\", 'cd', 'purple', '‚úã', 'franklin', 'homies', 'brew', '‚úîÔ∏è', 'missouri', 'ta', 'cellular', 'jewelry', 'reach', 'associates', 'obreg√≥n', 'punch', 'oops', 'shades', 'lauderdale', 'average', 'emojis', 'mason', 'highway', '@tvtelehit', 'thunderstorm', 'foodservice', 'bedrooms', 'williams', 'sabe', '#hollywood', 'dew', 'tip', 'sore', '#irvine', 'ultra', 'pregnant', 'kanye', 'regions', 'bars', 'trailer', 'tune', 'ignore', 'cop', '#goodmorning', 'riverside', 'division', \"someone's\", 'nail', '‚úàÔ∏è', 'bean', '101', '73¬∞f', 'papa', 'gm', 'northern', 'rate', 'tall', '1/2', 'cost', 'haircut', 'china', 'access', 'smells', 'palacio', 'kno', 'mvp', 'marathon', 'salty', 'dave', 'pueblo', 'booth', 'cuddle', '#life', 'cedar', 'fail', 'asian', 'kindred', 'content', 'etc', 'create', '@luke5sos', 'utah', 'photography', '84%', 'üçª', 'shame', 'somos', 'cousins', '#albany', 'burgers', '36', '13mph', 'decisions', 'casual', 'nash', 'colonial', 'mesa', 'üëã', 'catering', 'candidate', 'eyebrows', 'bowling', 'foundation', 'ryder', 'agency', 'drug', 'don‚Äôt', 'draft', 'josh', '@aicm3', 'genuine', 'supporting', 'favorites', 'nuestra', 'farmers', 'hates', '#newyorkcity', 'unos', 'louisiana', 'exciting', 'durante', 'clutch', 'naked', 'hum', 'sooooo', 'lock', 'killer', '#2', 'def', 'shrimp', '#alexandria', 'sonora', 'article', 'mixed', 'boutique', 'talkin', '#follow', 'competition', 'hermoso', 'canadian', 'cruise', 'application', 'cameron', 'messages', 'ot', 'seller', 'theme', 'arm', 'teaching', 'strength', 'corn', 'comida', 'resolved', 'hookah', 'boardwalk', 'cops', 'flash', 'strip', 'industrial', 'loyal', 'necesito', 'offer', 'deck', 'comedy', '66', 'freshman', 'üå¥', '¬ø', '#ny', 'vintage', 'quieres', '48', 'needa', 'upper', 'parte', 'allowed', 'bottles', 'workers', 'counsel', 'wireless', 'brands', 'sour', 'ayer', 'doors', 'huntington', 'basic', 'teachers', 'ole', 'pork', 'museo', 'ship', 'hahah', 'complex', '#coloradosprings', 'teller', 'beast', 'progress', 'yr', 'fill', 'packed', 'swimming', 'pinche', '81%', 'served', 'ideas', 'modern', '72¬∞f', 'idiot', '@zaynmalik', 'punta', 'fucks', 'dressed', 'sobre', 'jimmy', 'sneak', 'dishwasher', '#me', 'paying', 'tal', 'pit', 'painting', 'gross', '#wilmington', 'madero', 'raw', 'mommy', \"weren't\", 'mirror', 'carlos', 'tank', 'jazz', 'emotional', 'edition', 'beers', 'institute', 'heads', 'society', 'search', '95', 'piss', 'gosh', 'bonita', 'crib', 'fame', 'pieces', 'primer', 'signs', 'puedes', 'miller', '#dinner', 'prince', 'saber', 'oct', 'kicking', 'pine', 'treatment', 'inspired', 'darle', 'rts', 'estos', 'bilingual', 'victory', '82%', '70¬∞f', 'england', 'character', 'easier', 'decir', 'devil', 'mujer', 'ghost', 'bush', '#disney', 'patio', 'fog', '#wedding', 'cutie', '20th', 'lower', 'effect', 'jueves', 'steal', 'inches', 'lmaooo', 'facts', 'ramp', 'walt', 'edge', 'dating', 'soup', 'padre', 'babes', '#laarrolladoraenplbns', '75¬∞f', 'weekends', 'glasses', 'sheriff', 'passion', '300', '72', 'emoji', 'longest', 'üò•', 'seasons', 'hero', 'rare', 'esos', '#merrychristmas', 'defense', 'trugreen', 'eagle', '#healthwelfare', 'uk', '#fl', 'jokes', 'goodness', 'wnw', 'smiling', 'screaming', 'aunt', '10th', 'uber', '#saintlouis', '#tgif', 'tasting', '84', 'simply', '#oakland', 'pharmacist', 'easily', 'prices', 'poder', 'diner', '43', 'roof', 'niagara', 'ven', 'depth', 'roo', 'created', 'extremely', 'mfs', 'craft', '#bocaraton', 'effort', 'hangout', 'flag', 'üòñ', 'jail', '78%', 'rocky', '39', 'aquarium', '69', 'esperando', 'role', 'equipment', 'form', 'float', 's√©', '11:11', ':p', 'hampton', 'checked', 'rooftop', 'raining', 'myrtle', 'setting', 'tweeted', 'hudson', 'monster', 'colonia', 'accountable', 'lodge', 'imperial', 'became', 'mutual', 'tim', '#government', '1000', 'windows', 'prime', '77¬∞f', 'invite', 'rules', 'becoming', 'trynna', 'wm', 'arrested', 'silly', 'ah√≠', 'rick', 'tienen', 'qro', 'zero', 'puts', 'skills', 'cumple', 'tequila', 'breathe', '#birmingham', 'üåä', 'nervous', 'softball', 'ŸÅŸä', 'odio', '79¬∞f', 'lawn', '73%', 'wifi', 'thai', 'üçï', 're', 'pops', 'temple', 'helped', 'kicked', 'government', 'london', 'hearts', '#downtown', 'taught', 'üî•üî•', 'stayed', 'tracking', 'lado', 'prayers', 'jeans', 'camino', 'remind', 'cried', '33', 'blind', '#santaclara', 'toluca', 'seats', 'üèÄ', 'sigo', 'üôá', 'esas', 'army', 'homecoming', 'friendly', 'mientras', 'ferry', 'paris', 'produce', '76%', 'superior', 'vacaciones', 'capitol', 'farms', 'enthusiast', 'bite', 'feat', '#newark', 'hunt', 'hardest', 'path', 'comfortable', \":')\", '@niallofficial', '#birthday', 'whiskey', 'repost', 'chips', 'petty', '#m', 'material', 'whip', '@somoscd9', 'hiking', '97%', 'expected', 'barber', 'lindo', '68¬∞f', 'dem', 'dollars', '#sunday', 'col', 'chair', 'purchase', 'studying', 'funniest', 'üôã', 'steakhouse', 'cgi', 'fellow', 'morgan', 'triple', 'button', '#batonrouge', 'lived', '√©xico', 'covered', 'murder', 'knock', 'specialty', '#rent', '#mariobautista', 'raise', 'alot', 'ruin', 'chip', 'musical', 'veo', '#venabailar', 'loose', 'chain', 'parties', 'lobster', 'rainbow', 've', 'ssw', 'crown', 'conmigo', 'attendant', 'ray', '79%', 'winery', 'otr', 'cast', 'bartender', 'springfield', 'paseo', 'primera', '#jerseycity', '#mariobautistaredes', 'deja', 'pot', '83%', 'flat', 'cara', 'products', 'teen', 'benefits', 'built', 'marry', 'dessert', 'towards', 'davis', 'reminds', '#dayton', 'motor', 'flip', 'crime', 'ser√°', 'ordered', 'pal', 'fired', 'das', 'desert', 'shirts', 'üéÑ', 'helper', 'boom', 'mais', 'guerrero', 'repeat', '79', 'crossfit', 'igual', 'thot', 'crab', 'tiny', '2013', 'navidad', 'viendo', 'roads', 'messed', '#plano', 'rule', 'memory', 'rep√∫blica', 'mrs.', '#database', 'garage', 'justice', 'whether', 'net', 'breath', 'tony', 'testing', 'üò©üò©üò©', 'snowing', 'facetime', 'option', 'üéß', 'eu', '#empire', '@villalobossebas', 'mississippi', 'heck', 'prolly', '***', 'twins', 'captain', 'illegal', '#work', 'impossible', 'items', 'shoe', '52', 'jr.', '9th', 'brandon', '34', 'magnitude', '#longbeach', '71', 'smooth', 'breaks', 'finger', 'champions', 'constantly', '#nblmarianavoto', 'multiple', 'üòíüòí', '#productmgmt', '#desmoines', 'jajajajaja', 'thunder', 'liquor', '73', '70%', 'lift', 'hamilton', 'blowing', 'teatro', \"they'll\", 'dicen', '5sos', 'cielo', '#photooftheday', 'vale', '...?', 'verizon', 'protection', 'irvine', 'wat', 'mon', 'accept', 'telemetry', '#craftbeer', 'male', 'worship', 'awards', 'dar', '46', 'ik', 'hook', 'keys', 'closing', 'industry', 'absolute', 'damage', 'section', 'animals', 'suit', 'restaurante', 'jason', 'fifth', 'crystal', 'vibe', 'claims', 'jr', 'swing', 'ilysm', 'elite', 'handler', 'freak', 'motivation', 'yu', 'au', 'mid', '64', '32801', 'strawberry', 'ahh', 'java', 'warriors', 'carry', 'electronic', 'birds', 'usual', 'stream', 'mode', 'rush', 'admin', '19th', 'bagger', 'stole', 'charger', 'peanut', 'eastern', 'mujeres', 'mornings', 'independence', 'johnny', '89%', '#mexicocity', 'bearing', 'mail', 'fishing', 'tasty', 'coolest', 'exist', 'cutting', 'wrote', 'connecticut', 'mango', '@cinemex', 'martes', '67', '66¬∞f', 'kills', 'decent', 'awhile', 'cinemas', 'boots', 'tres', 'shawn', 'electric', 'relationships', 'banana', 'plate', '#rn', 'inspiration', 'athletic', 'iron', 'disappointed', 'newest', '#practicewithus', 'henry', 'hmm', 'manufacturing', 'ap', 'fingers', 'recruiter', '88%', 'nothin', '37', 'tijuana', 'jacob', 'likely', '...‚Ä¶', '57', 'üí©', 'awww', 'homes', 'matters', '74%', 'bible', 'header', 'planned', 'tips', 'penn', 'zaragoza', 'flower', '63¬∞f', 'blessings', 'z', 'notre', 'socks', 'mistake', 'tomato', 'pleasant', 'expo', 'higher', 'islands', 'mass', 'kitty', 'aide', 'nfl', 'sausage', 'hide', 'premium', 'rent', 'dms', 'largest', 'andrew', \"men's\", 'laundry', 'scarborough', 'oaks', 'seguir', 'ko', 'maria', 'bills', 'üò¥üò¥', '#chicken', 'eagles', '#lunch', 'belly', 'disturbance', '.?', 'encanta', '#yum', 'wave', '#wrestlemania', '@harry_styles', '77', 'accidentally', 'toast', 'ou', '44', '#driver', 'steps', 'accurate', 'adam', 'joint', 'quintana', 'chiapas', 'notes', '24/7', 'jim', 'clinton', 'breast', 'ja', '‚ÄºÔ∏è', '38', '#friday', 'rosa', 'september', 'jurassic', 'results', '14mph', 'relaxing', '42', '#suspiciousperson', 'spread', '1in', 'guadalupe', 'maine', 'bend', 'ace', 'happier', 'smoked', 'reunited', 'trabajar', 'baja', 'stressed', '#purchasing', '#thewalkingdead', '#utilities', 'traveling', 'meters', '#hillsboro', 'alma', '@midnight', 'nike', 'sweat', '68', '13th', 'gifts', 'üò∂', 'fotos', 'pasta', '54', 'howard', 'crescent', 'pumped', 'prevention', 'intel', 'paz', '#wvwx', '\"\"', 'fml', 'dining', 'üò≠üòÇ', 'se√±or', 'jacket', 'omaha', 'peter', 'gusto', 'shits', 'üôÖ', 'pc', 'hambre', 'ruined', 'rad', '#votefifthharmony', 'response', 'tennis', 'bebe', 'ol', 'escape', 'pi', 'm√≠', 'attractive', 'hurting', 'besos', 'greater', 'grant', 'salir', 'hombre', 'quickly', 'uptown', 'swift', '#cibc', '#amazing', 'olive', '¬∞c', 'üòäüòä', 'noticed', 'dec', 'campeche', 'helps', 'greek', '64¬∞f', '#santamonica', 'complain', 'ustedes', 'popular', 'accenture', 'haciendo', 'hq', 'exhausted', '@arianagrande', 'amber', 'correct', 'hunter', 'nowhere', 'gn', 'fm', 'dale', 'daniel', 'finale', 'Ÿà', 'cap', 'appointment', '#qa', 'waffle', 'lp', '#calgary', '&&', 'macy‚Äôs', 'mister', 'lemon', '16th', 'r√≠o', 'wolf', 'ÿßŸÑŸÑŸá', 'lyrics', 'http://t.co/zumecym652', 'rancho', 'roasted', 'laguna', 'hiv', 'fancy', 'ellos', 'tie', 'thirsty', 'purpose', '#ottawa', 'loop', 'tuxtla', 'interest', 'norfolk', 'gulf', '#hiphop', 'culture', 'dedicated', 'potato', 'lovin', '#washingtondc', 'üëê', 'advanced', 'cantina', '#workout', 'ojos', 'chelsea', 'opportunities', 'angels', 'hating', '#socialsciences', '........', '80%', '#wine', 'stylist', 'possibly', '56', '7:30', '√°lvaro', 'seed', '68%', '#fayetteville', 'coconut', 'gear', 'reunion', 'bands', '62', \"c'est\", 'malibu', 'üëâ', 'sandy', 'üòö', 'disgusting', 'hes', 'porn', 'nephew', 'generation', 'cultural', 'contest', '41', 'hermosillo', 'rolls', 'jake', '#spokane', \"he'll\", 'landing', 'bra', '#breakfast', 'agua', '2012', 'whbm', 'deserves', 'ottawa', '@nickiminaj', 'ashley', 'wheel', 'airlines', 'praise', 'norte', 'serving', 'bby', 'confidencial', 'yucat√°n', 'numbers', 'naw', 'vi', 'cancelled', 'bass', '#bellevue', 'pavilion', 'awareness', 'vallarta', 'lesson', 'diamonds', 'sarah', '65%', '!!!‚Ä¶', 'glen', 'canal', 'walks', 'shawty', 'chapter', 'lager', 'alto', 'boi', 'metropolitan', '#riverside', 'vivir', 'peeps', 'throat', '#energy', 'campaign', 'cuernavaca', 'arkansas', 'collision', 'monument', 'banquet', '@5sos', 'donut', 'inbound', 'katy', 'donate', 'waterfront', \"chico's\", 'panama', 'shes', 'dies', 'maple', 'term', 'jeff', '81¬∞f', 'patch', '61', 'status', 'artists', 'garc√≠a', '#mtvhottest', '#housebusareacheck', 'whose', 'released', 'magical', '#goodtimes', '75%', 'partners', 'bags', 'cus', 'standard', '#tacoma', '71¬∞f', 'exam', 'ps', 'manor', 'marriage', 'counting', '74', 'graphic', 'ultimate', 'fuera', 'faster', 'navy', 'glory', 'assist', 'eric', '51', 'electrical', 'manchester', 'brick', 'hire', 'besides', 'clock', 'savannah', '53', 'beating', '58', '#glendale', '76¬∞f', '#sugarland', '17th', 'volleyball', 'garbage', 'mami', 'somehow', 's√∫per', 'admit', 'stoked', 'nuts', \"life's\", 'costa', 'pastor', 'prove', 'duh', 'relations', 'wise', 'idgaf', 'license', 'honored', 'drove', \"how's\", 'lion', 'pachuca', 'flavor', 'drops', 'aun', 'cheap', 'negative', 'wilson', 'thick', 'awe', 'mercado', '60%', '95%', '#ootd', 'stunning', '77%', ':-(', 'sometime', 'midtown', 'dime', 'northside', '#media', 'salud', 'üòòüòòüòò', 'skip', '#sky', 'han', 'skinny', 'laughed', 'burning', 'anthony', '#edmonton', 'equipo', '9mi', 'waters', 'edmonton', 'craig', 'tenemos', 'monkey', 'üíÖ', 'phones', 'haber', 'texted', 'bestie', '86%', '49', 'prison', 'rockin', 'professor', 'salem', 'nvm', 'dare', 'guitar', 'operating', '76', 'sub', '::', 'cena', 'smiles', 'claim', 'pathologist', 'emotions', 'mag', 'niece', 'toy', '#alpharetta', 'communications', '#saturday', 'rochester', 'allen', 'pediatric', 'novio', 'ink', 'brian', '78', '61¬∞f', 'hyatt', 'familiar', 'creepy', '#home', 'ana', 'instituto', 'launch', 'muchos', '63', 'loan', 'birth', '#nc', 'immediate', '#wichita', 'mes', '#jackson', '47', '#yummy', 'logan', 'turkey', '69¬∞f', 'roommate', 'digo', 'prayer', 'aug', 'bench', 'fixed', 'twist', 'nite', 'humble', 'copy', 'difficult', 'carrier', 'nearly', 'charges', 'comin', 'mane', '‚Äù‚Äù', '6:30', 'levi', 'hosted', '#reno', 'consider', 'passing', 'spell', 'leads', 'üò†', 'dads', 'luego', 'beard', 'charged', 'easter', '69%', 'hottest', 'duty', '#disneyland', '63%', '74¬∞f', 'grupo', 'mills', \"people's\", 'filter', 'metal', 'racing', 'potential', 'perform', 'tbt', 'morelia', 'inventory', 'hacen', 'ramen', '71%', 'alpha', 'racist', 'internal', '#overlandpark', 'üê∂', 'homeless', '78¬∞f', 'kyle', 'switch', 'diet', 'israel', 'wo', '66%', 'quebec', 'rica', 'trainer', 'ear', 'environmental', 'image', '#brunch', 'snack', 'ann', 'meek', 'calle', 'aspen', '‚òïÔ∏è', 'wsw', '82¬∞f', 'auburn', \"could've\", 'figured', 'bedroom', 'dates', 'üíÄüíÄüíÄ', 'yrs', 'gustavo', 'carter', 'century', 'tongue', 'pig', 'describe', 'bother', '#cute', '7pm', '@waltdisneyworld', 'locked', 'discount', 'commission', 'hugs', '85', 'aguascalientes', '#beauty', 'sterling', '11th', 'value', 'vanilla', 'ms.', '#dublin', 'mira', 'grocery', 'breathing', 'une', 'corp', 'suspicious', '#buffalo', '14th', 'üíì', 'gordon', '57%', 'delaware', 'distance', 'mechanical', 'virgin', 'mam√°', 'remix', 'chapel', 'guns', '#nola', 'mortgage', '72%', 'shack', 'rapids', '67%', 'ted', 'üòÇüíÄ', 'degree', 'asks', 'cubs', 'universe', ';-)', 'rouge', 'woes', 'limited', 'doesnt', 'employee', 'allow', 'hacienda', 'spicy', 'indeed', 'grille', '√©l', 'lifestyle', 'hermano', 'weekly', 'ears', \":'(\", 'root', 'hablar', 'walls', 'depression', 'ce', 'lexington', 'sinaloa', \"god's\", 'hist√≥rico', 'honda', '#weekend', 'debate', 'adams', 'homemade', 'dock', '#green', 'kisses', 'background', 'maya', 'horror', 'packing', 'thug', 'rescue', 'i-95', 'hosting', 'index', '400', 'facility', '#cdmx', '12th', 'toyota', 'pretend', '#franklin', 'pole', 'cheat', '#simon', 'received', 'anxiety', 'perfectly', 'peep', 'enter', 'theres', 'fa', 'aid', 'isla', '#colorado', 'llegar', 'bucket', 'crossing', 'saving', 'magazine', 'rodeo', 'several', 'tiger', 'insta', 'scottsdale']\n"
     ]
    }
   ],
   "source": [
    "top_5k = open('top_5k_twitter_2015.txt')\n",
    "top_5k_list = []\n",
    "for line in top_5k:\n",
    "    tokenized_top_5k = ' '.join(tokenizeRawTweetText(line))\n",
    "    tokenized_top_5k_list = tokenized_top_5k.split(' ')\n",
    "    top_5k_list.append(tokenized_top_5k_list[0])\n",
    "print(top_5k_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drinking a Smoglifter by @brashbrewingco @ Spoiled Rotten Grayton Beach, FL ‚Äî http://t.co/h9LJZwCrm9\n",
      "[[]]\n",
      "[]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b90b73fd4a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mdiversity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_tokenized_tweet_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_open_class\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnumber_of_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mn_slex_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_slex\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnumber_of_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mttr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnumber_of_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "inp_file = open('alcohol_tweets_4k.txt')\n",
    "#inp_file = open('tweets.txt')\n",
    "\n",
    "oup_file = open(\"t_t_p.txt\", \"w\") \n",
    "\n",
    "\n",
    "open_class=['N','A','V','R']\n",
    "\n",
    "lexical_density_ratio_list = []\n",
    "n_slex_ratio_list = []\n",
    "ttr_list = []\n",
    "tokenized_tweet_list=[]\n",
    "#oup_file = open(\"tweets_tokenized.txt\", \"w\") \n",
    "for line in inp_file:\n",
    "    #print(\"---\")\n",
    "    tokenized_tweet_list=[]\n",
    "    tokenized_tweet = ' '.join(tokenizeRawTweetText(line))\n",
    "    print(tokenized_tweet)\n",
    "    #tokenized_tweet_list = tokenized_tweet.split(' ')\n",
    "    #unique_tokenized_tweet_list = set(tokenized_tweet_list)\n",
    "    \n",
    "    \n",
    "    out = runtagger_parse([tokenized_tweet])\n",
    "    print(out)\n",
    "    number_of_words =len(out[0])\n",
    "    print(out[0])\n",
    "    n_open_class = 0\n",
    "    n_slex = 0\n",
    "    for i in range(number_of_words):\n",
    "        tokenized_tweet_list.append(out[0][i][0])\n",
    "        if out[0][i][1] in open_class : \n",
    "            n_open_class += 1\n",
    "    oup_file.write(line.strip()+'\\t'+str(tokenized_tweet_list)+'\\t'+str(out[0]) + '\\n')\n",
    "    for i in range(number_of_words):\n",
    "        if out[0][i][0].lower() not in top_5k_list :\n",
    "            #print(out[0][i][0])\n",
    "            n_slex +=1\n",
    "            \n",
    "    unique_tokenized_tweet_list = set(tokenized_tweet_list)\n",
    "    diversity = len(unique_tokenized_tweet_list)\n",
    "    \n",
    "    ratio = n_open_class/number_of_words\n",
    "    n_slex_ratio = n_slex/number_of_words\n",
    "    ttr = diversity/number_of_words\n",
    "    lexical_density_ratio_list.append(ratio)\n",
    "    n_slex_ratio_list.append(n_slex_ratio)\n",
    "    ttr_list.append(ttr)\n",
    "    #print(unique_tokenized_tweet_list, diversity, number_of_words)\n",
    "\n",
    "    \n",
    "    #density_file.write(line.strip() +'\\t'+str(n_open_class)+ '/'+str(number_of_words)+ '\\n')\n",
    "    \n",
    "    \n",
    "inp_file.close()\n",
    "\n",
    "oup_file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_index(list_in):\n",
    "\n",
    "    sort_ = []\n",
    "    for i in range (len(list_in)): \n",
    "        max_index = np.argmax(list_in)\n",
    "        list_in[max_index] = list_in[max_index]-2 \n",
    "        sort_.append(max_index)\n",
    "    for i in range (len(list_in)): \n",
    "        list_in[i] =list_in[i] +2\n",
    "    return sort_\n",
    "\n",
    "def file_sorter(content,out_file, in_ratio, sort_list):\n",
    "    for i in sort_list : \n",
    "        out_file.write(content[i].strip() + '\\t' + str(in_ratio[i]) + '\\n')\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_file = open('alcohol_tweets_4k.txt')\n",
    "#inp_file = open('tweets.txt')\n",
    "\n",
    "density_file = open(\"density.txt\", \"w\")\n",
    "sophistication_file = open(\"sophistication.txt\", \"w\")\n",
    "diversity_file = open(\"diversity.txt\", \"w\")\n",
    "\n",
    "lexical_index_sort = sort_index(lexical_density_ratio_list)\n",
    "n_slex_index_sort = sort_index(n_slex_ratio_list)\n",
    "ttr_index_sort = sort_index(ttr_list)\n",
    "content = inp_file.readlines()\n",
    "file_sorter(content,density_file,lexical_density_ratio_list,lexical_index_sort)\n",
    "file_sorter(content,sophistication_file,n_slex_ratio_list,n_slex_index_sort)\n",
    "file_sorter(content,diversity_file,ttr_list,ttr_index_sort)\n",
    "\n",
    "inp_file.close()\n",
    "\n",
    "density_file.close()\n",
    "sophistication_file.close()\n",
    "diversity_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "rll , pll = stats.pearsonr(lexical_density_ratio_list,lexical_density_ratio_list)\n",
    "rln , pln = stats.pearsonr(lexical_density_ratio_list,n_slex_ratio_list)\n",
    "rlt , plt = stats.pearsonr(lexical_density_ratio_list,ttr_list)\n",
    "\n",
    "rnl , pnl = stats.pearsonr(n_slex_ratio_list,lexical_density_ratio_list)\n",
    "rnn , pnn = stats.pearsonr(n_slex_ratio_list,n_slex_ratio_list)\n",
    "rnt , pnt = stats.pearsonr(n_slex_ratio_list,ttr_list)\n",
    "\n",
    "rtl , ptl = stats.pearsonr(ttr_list,lexical_density_ratio_list)\n",
    "rtn , ptn = stats.pearsonr(ttr_list,n_slex_ratio_list)\n",
    "rtt , ptt = stats.pearsonr(ttr_list,ttr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_file = open(\"correlations.txt\", \"w\") \n",
    "\n",
    "correlations_file.write('\\t' +'\\t' +'\\t' +'\\t'+'\\t'+'density' + '\\t' + '\\t'+ '\\t'+'sophistication' + '\\t' + '\\t'+ '\\t'+'diversity'+  '\\n')\n",
    "correlations_file.write('density' +'\\t' + '\\t'+'\\t' +'\\t'+str(rll) + '\\t' +str(rln) + '\\t' +str(rlt)+  '\\n')\n",
    "correlations_file.write('sophistication' +'\\t' +'\\t' +str(rnl) + '\\t' +str(rnn) + '\\t' +str(rnt)+  '\\n')\n",
    "correlations_file.write('diversity' +'\\t' +'\\t' + '\\t'+str(rtl) + '\\t' +str(rtn) + '\\t' +str(rtt)+  '\\n'+'\\n'+'\\n')\n",
    "correlations_file.write(\"The values on the diagnal are all ~1 which is due to self corelation. The result of the table above shows that the linear corelation\"+'\\n')\n",
    "correlations_file.write(\"between sophistication and dencity is higher than the other two. Diversity and density have the lowest corelation, because having Noun,Verb,adj \"+'\\n')\n",
    "correlations_file.write(\"or adv does not change the diversity of the tweet. On the other hand, the results shoes that if we have more of the open-class words, the chance \"+'\\n')\n",
    "correlations_file.write(\"of having more sophisticated words goes up.\")\n",
    "correlations_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
