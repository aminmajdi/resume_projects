{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter POS Tagging \n",
    "The goal of this tutorial is to introduce a a Part-of-Speech (POS) tagger developed for tweets which was released as part of the [TweetNLP](https://www.ark.cs.cmu.edu/TweetNLP/) toolkit. The code is written in Java and the python wrapper for the tokenization is from [this](https://github.com/myleott/ark-twokenize-py) github repository. This tutorial has code from the [TweetNLP](https://github.com/brendano/ark-tweet-nlp/) github repository as well as the python wrapper from [this](https://github.com/ianozsvald/ark-tweet-nlp-python) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "- POS tagging involves identifying part-of-speech of tokens in a given text. This can be viewed as a task of labeling the sentence w_1, w_2, ....., w_n with pos tags, one for each word: t_1, t_2, ...., t_n.\n",
    "- The 8 common parts of speech for english language are:\n",
    "  1. Noun\n",
    "  2. Verb\n",
    "  3. Pronoun\n",
    "  4. Preposition\n",
    "  5. Adverb\n",
    "  6. Conjuction\n",
    "  7. Participle\n",
    "  8. Article  \n",
    "- Twitter data is different from standard language data in that there are tokens such as #, @, emoticons, URLs, etc. So the tagset for twitter needs to incorporate the tags for these new tokens. The tags that are used to annotate tweets are as follows:\n",
    "\n",
    "<img src=\"pos_tags.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments\n",
    "- This tutorial covers how to accomplish the task of POS tagging for twitter data based on this paper: https://aclanthology.org/P11-2008.pdf\n",
    "- The nature of twitter data poses challenges in using standard POS taggers. The paper develops the above tagset for twitter to include tags for words that are not commonly encountered in language outside of twitter. \n",
    "- Around 1,800 tweets were manually annotated with corresponding pos tags.\n",
    "- Conditional Random Fields (CRFs) were used with features specific to twitter POS tagging. The features for the CRF are below (see paper for more details):\n",
    "  - Twitter orthography - these features are rules that detect @, #, and URls.\n",
    "  - Names - these features check for names from a dictionary of compiled tokens which are frequently capitalized.\n",
    "  - Traditional Tag Dictionary - these are features for all tags that occur in PTB.\n",
    "  - Distributional Similarity - these features are constructed from the successor and predecessor probabilities for the 10,000 most common terms.\n",
    "  - Phonetic normalization - words are normalized to ignore alternate spellings of words using the Metaphone algorithm; e.x.{thangs, thanks, thanksss, thanx, thinks, thnx} are mapped to 0NKS.\n",
    "- 1827 tweets that are annotated are divided into training set of 1000 tweets, dev set of 327 tweets, and test set of 500 tweets. The results of the tagger incorporating the above features are compared with the standard Stanford Tagger and using the above feature set for twitter data reduces error by about 25%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "- You will need to download the POS tagger from https://code.google.com/archive/p/ark-tweet-nlp/downloads\n",
    "- This requires Java 6. https://www.oracle.com/java/technologies/java-platform.html\n",
    "- Place this ipython notebook that has python wrappers inside the ark-tweet-nlp-0.3.2 folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "try:\n",
    "    from html.parser import HTMLParser\n",
    "except ImportError:\n",
    "    from HTMLParser import HTMLParser\n",
    "\n",
    "try:\n",
    "    import html\n",
    "except ImportError:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Contractions = re.compile(u\"(?i)(\\w+)(n['’′]t|['’′]ve|['’′]ll|['’′]d|['’′]re|['’′]s|['’′]m)$\", re.UNICODE)\n",
    "Whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
    "punctChars = r\"['\\\"“”‘’.?!…,:;]\"\n",
    "punctSeq   = r\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\n",
    "entity     = r\"&(?:amp|lt|gt|quot);\" # see more here https://www.w3schools.com/html/html_entities.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_or(*items):\n",
    "    return '(?:' + '|'.join(items) + ')'\n",
    "\n",
    "urlStart1  = r\"(?:https?://|\\bwww\\.)\"\n",
    "commonTLDs = r\"(?:com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xxx)\"\n",
    "ccTLDs = r\"(?:ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|\" + \\\n",
    "r\"bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|\" + \\\n",
    "r\"er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|\" + \\\n",
    "r\"hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|\" + \\\n",
    "r\"lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|\" + \\\n",
    "r\"nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|\" + \\\n",
    "r\"sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|\" + \\\n",
    "r\"va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)\"\t#TODO: remove obscure country domains?\n",
    "urlStart2  = r\"\\b(?:[A-Za-z\\d-])+(?:\\.[A-Za-z0-9]+){0,3}\\.\" + regex_or(commonTLDs, ccTLDs) + r\"(?:\\.\"+ccTLDs+r\")?(?=\\W|$)\"\n",
    "urlBody    = r\"(?:[^\\.\\s<>][^\\s<>]*?)?\"\n",
    "urlExtraCrapBeforeEnd = regex_or(punctChars, entity) + \"+?\"\n",
    "urlEnd     = r\"(?:\\.\\.+|[<>]|\\s|$)\"\n",
    "url        = regex_or(urlStart1, urlStart2) + urlBody + \"(?=(?:\"+urlExtraCrapBeforeEnd+\")?\"+urlEnd+\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "monetary = r\"\\$([0-9]+)?\\.?([0-9]+)?\"\n",
    "timeLike   = r\"\\d+(?::\\d+){1,2}\"\n",
    "numberWithCommas = r\"(?:(?<!\\d)\\d{1,3},)+?\\d{3}\" + r\"(?=(?:[^,\\d]|$))\"\n",
    "numComb = u\"[\\u0024\\u058f\\u060b\\u09f2\\u09f3\\u09fb\\u0af1\\u0bf9\\u0e3f\\u17db\\ua838\\ufdfc\\ufe69\\uff04\\uffe0\\uffe1\\uffe5\\uffe6\\u00a2-\\u00a5\\u20a0-\\u20b9]?\"\n",
    "boundaryNotDot = regex_or(\"$\", r\"\\s\", r\"[“\\\"?!,:;]\", entity)\n",
    "aa1  = r\"(?:[A-Za-z]\\.){2,}(?=\" + boundaryNotDot + \")\"\n",
    "aa2  = r\"[^A-Za-z](?:[A-Za-z]\\.){1,}[A-Za-z](?=\" + boundaryNotDot + \")\"\n",
    "standardAbbreviations = r\"\\b(?:[Mm]r|[Mm]rs|[Mm]s|[Dd]r|[Ss]r|[Jj]r|[Rr]ep|[Ss]en|[Ss]t)\\.\"\n",
    "arbitraryAbbrev = regex_or(aa1, aa2, standardAbbreviations)\n",
    "separators  = \"(?:--+|―|—|~|–|=)\"\n",
    "decorations = u\"(?:[♫♪]+|[★☆]+|[♥❤♡]+|[\\u2639-\\u263b]+|[\\ue001-\\uebbb]+)\"\n",
    "thingsThatSplitWords = r\"[^\\s\\.,?\\\"]\"\n",
    "embeddedApostrophe = thingsThatSplitWords+r\"+['’′]\" + thingsThatSplitWords + \"*\"\n",
    "normalEyes = \"[:=]\" # 8 and x are eyes but cause problems\n",
    "wink = \"[;]\"\n",
    "noseArea = \"(?:|-|[^a-zA-Z0-9 ])\" # doesn't get :'-(\n",
    "happyMouths = r\"[D\\)\\]\\}]+\"\n",
    "sadMouths = r\"[\\(\\[\\{]+\"\n",
    "tongue = \"[pPd3]+\"\n",
    "otherMouths = r\"(?:[oO]+|[/\\\\]+|[vV]+|[Ss]+|[|]+)\" # remove forward slash if http://'s aren't cleaned\n",
    "\n",
    "# mouth repetition examples:\n",
    "# @aliciakeys Put it in a love song :-))\n",
    "# @hellocalyclops =))=))=)) Oh well\n",
    "\n",
    "# myleott: try to be as case insensitive as possible, but still not perfect, e.g., o.O fails\n",
    "#bfLeft = u\"(♥|0|o|°|v|\\\\$|t|x|;|\\u0ca0|@|ʘ|•|・|◕|\\\\^|¬|\\\\*)\".encode('utf-8')\n",
    "bfLeft = u\"(♥|0|[oO]|°|[vV]|\\\\$|[tT]|[xX]|;|\\u0ca0|@|ʘ|•|・|◕|\\\\^|¬|\\\\*)\"\n",
    "bfCenter = r\"(?:[\\.]|[_-]+)\"\n",
    "bfRight = r\"\\2\"\n",
    "s3 = r\"(?:--['\\\"])\"\n",
    "s4 = r\"(?:<|&lt;|>|&gt;)[\\._-]+(?:<|&lt;|>|&gt;)\"\n",
    "s5 = \"(?:[.][_]+[.])\"\n",
    "# myleott: in Python the (?i) flag affects the whole expression\n",
    "#basicface = \"(?:(?i)\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "basicface = \"(?:\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "\n",
    "eeLeft = r\"[＼\\\\ƪԄ\\(（<>;ヽ\\-=~\\*]+\"\n",
    "eeRight= u\"[\\\\-=\\\\);'\\u0022<>ʃ）/／ノﾉ丿╯σっµ~\\\\*]+\"\n",
    "eeSymbol = r\"[^A-Za-z0-9\\s\\(\\)\\*:=-]\"\n",
    "eastEmote = eeLeft + \"(?:\"+basicface+\"|\" +eeSymbol+\")+\" + eeRight\n",
    "\n",
    "oOEmote = r\"(?:[oO]\" + bfCenter + r\"[oO])\"\n",
    "\n",
    "emoticon = regex_or(\n",
    "        # Standard version  :) :( :] :D :P\n",
    "        \"(?:>|&gt;)?\" + regex_or(normalEyes, wink) + regex_or(noseArea,\"[Oo]\") + regex_or(tongue+r\"(?=\\W|$|RT|rt|Rt)\", otherMouths+r\"(?=\\W|$|RT|rt|Rt)\", sadMouths, happyMouths),\n",
    "\n",
    "        # reversed version (: D:  use positive lookbehind to remove \"(word):\"\n",
    "        # because eyes on the right side is more ambiguous with the standard usage of : ;\n",
    "        regex_or(\"(?<=(?: ))\", \"(?<=(?:^))\") + regex_or(sadMouths,happyMouths,otherMouths) + noseArea + regex_or(normalEyes, wink) + \"(?:<|&lt;)?\",\n",
    "\n",
    "        #inspired by http://en.wikipedia.org/wiki/User:Scapler/emoticons#East_Asian_style\n",
    "        eastEmote.replace(\"2\", \"1\", 1), basicface,\n",
    "        # iOS 'emoji' characters (some smileys, some symbols) [\\ue001-\\uebbb]\n",
    "        # TODO should try a big precompiled lexicon from Wikipedia, Dan Ramage told me (BTO) he does this\n",
    "\n",
    "        # myleott: o.O and O.o are two of the biggest sources of differences\n",
    "        #          between this and the Java version. One little hack won't hurt...\n",
    "        oOEmote\n",
    ")\n",
    "\n",
    "Hearts = \"(?:<+/?3+)+\" #the other hearts are in decorations\n",
    "\n",
    "Arrows = regex_or(r\"(?:<*[-―—=]*>+|<+[-―—=]*>*)\", u\"[\\u2190-\\u21ff]+\")\n",
    "\n",
    "Hashtag = \"#[a-zA-Z0-9_]+\"\n",
    "AtMention = \"[@＠][a-zA-Z0-9_]+\"\n",
    "\n",
    "Bound = r\"(?:\\W|^|$)\"\n",
    "Email = regex_or(\"(?<=(?:\\W))\", \"(?<=(?:^))\") + r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}(?=\" +Bound+\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be tokenizing using these regexps as delimiters\n",
    "# Additionally, these things are \"protected\", meaning they shouldn't be further split themselves.\n",
    "Protected  = re.compile(\n",
    "    regex_or(\n",
    "        Hearts,\n",
    "        url,\n",
    "        Email,\n",
    "        timeLike,\n",
    "        monetary,\n",
    "        numberWithCommas,\n",
    "        numComb,\n",
    "        emoticon,\n",
    "        Arrows,\n",
    "        entity,\n",
    "        punctSeq,\n",
    "        arbitraryAbbrev,\n",
    "        separators,\n",
    "        decorations,\n",
    "        embeddedApostrophe,\n",
    "        Hashtag,\n",
    "        AtMention), re.UNICODE)\n",
    "\n",
    "# Edge punctuation\n",
    "# Want: 'foo' => ' foo '\n",
    "# While also:   don't => don't\n",
    "# the first is considered \"edge punctuation\".\n",
    "# the second is word-internal punctuation -- don't want to mess with it.\n",
    "# BTO (2011-06): the edgepunct system seems to be the #1 source of problems these days.\n",
    "# I remember it causing lots of trouble in the past as well.  Would be good to revisit or eliminate.\n",
    "\n",
    "# Note the 'smart quotes' (http://en.wikipedia.org/wiki/Smart_quotes)\n",
    "#edgePunctChars    = r\"'\\\"“”‘’«»{}\\(\\)\\[\\]\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunctChars    = u\"'\\\"“”‘’«»{}\\\\(\\\\)\\\\[\\\\]\\\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunct    = \"[\" + edgePunctChars + \"]\"\n",
    "notEdgePunct = \"[a-zA-Z0-9]\" # content characters\n",
    "offEdge = r\"(^|$|:|;|\\s|\\.|,)\"  # colon here gets \"(hello):\" ==> \"( hello ):\"\n",
    "EdgePunctLeft  = re.compile(offEdge + \"(\"+edgePunct+\"+)(\"+notEdgePunct+\")\", re.UNICODE)\n",
    "EdgePunctRight = re.compile(\"(\"+notEdgePunct+\")(\"+edgePunct+\"+)\" + offEdge, re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitEdgePunct(input):\n",
    "    input = EdgePunctLeft.sub(r\"\\1\\2 \\3\", input)\n",
    "    input = EdgePunctRight.sub(r\"\\1 \\2\\3\", input)\n",
    "    return input\n",
    "\n",
    "# The main work of tokenizing a tweet.\n",
    "def simpleTokenize(text):\n",
    "\n",
    "    # Do the no-brainers first\n",
    "    splitPunctText = splitEdgePunct(text)\n",
    "\n",
    "    textLength = len(splitPunctText)\n",
    "\n",
    "    # BTO: the logic here got quite convoluted via the Scala porting detour\n",
    "    # It would be good to switch back to a nice simple procedural style like in the Python version\n",
    "    # ... Scala is such a pain.  Never again.\n",
    "\n",
    "    # Find the matches for subsequences that should be protected,\n",
    "    # e.g. URLs, 1.0, U.N.K.L.E., 12:53\n",
    "    bads = []\n",
    "    badSpans = []\n",
    "    for match in Protected.finditer(splitPunctText):\n",
    "        # The spans of the \"bads\" should not be split.\n",
    "        if (match.start() != match.end()): #unnecessary?\n",
    "            bads.append( [splitPunctText[match.start():match.end()]] )\n",
    "            badSpans.append( (match.start(), match.end()) )\n",
    "\n",
    "    # Create a list of indices to create the \"goods\", which can be\n",
    "    # split. We are taking \"bad\" spans like\n",
    "    #     List((2,5), (8,10))\n",
    "    # to create\n",
    "    #     List(0, 2, 5, 8, 10, 12)\n",
    "    # where, e.g., \"12\" here would be the textLength\n",
    "    # has an even length and no indices are the same\n",
    "    indices = [0]\n",
    "    for (first, second) in badSpans:\n",
    "        indices.append(first)\n",
    "        indices.append(second)\n",
    "    indices.append(textLength)\n",
    "\n",
    "    # Group the indices and map them to their respective portion of the string\n",
    "    splitGoods = []\n",
    "    for i in range(0, len(indices), 2):\n",
    "        goodstr = splitPunctText[indices[i]:indices[i+1]]\n",
    "        splitstr = goodstr.strip().split(\" \")\n",
    "        splitGoods.append(splitstr)\n",
    "\n",
    "    #  Reinterpolate the 'good' and 'bad' Lists, ensuring that\n",
    "    #  additonal tokens from last good item get included\n",
    "    zippedStr = []\n",
    "    for i in range(len(bads)):\n",
    "        zippedStr = addAllnonempty(zippedStr, splitGoods[i])\n",
    "        zippedStr = addAllnonempty(zippedStr, bads[i])\n",
    "    zippedStr = addAllnonempty(zippedStr, splitGoods[len(bads)])\n",
    "\n",
    "    # BTO: our POS tagger wants \"ur\" and \"you're\" to both be one token.\n",
    "    # Uncomment to get \"you 're\"\n",
    "    #splitStr = []\n",
    "    #for tok in zippedStr:\n",
    "    #    splitStr.extend(splitToken(tok))\n",
    "    #zippedStr = splitStr\n",
    "\n",
    "    return zippedStr\n",
    "\n",
    "\n",
    "def addAllnonempty(master, smaller):\n",
    "    for s in smaller:\n",
    "        strim = s.strip()\n",
    "        if (len(strim) > 0):\n",
    "            master.append(strim)\n",
    "    return master\n",
    "\n",
    "# \"foo   bar \" => \"foo bar\"\n",
    "def squeezeWhitespace(input):\n",
    "    return Whitespace.sub(\" \", input).strip()\n",
    "\n",
    "# Final pass tokenization based on special patterns\n",
    "def splitToken(token):\n",
    "    m = Contractions.search(token)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [token]\n",
    "\n",
    "# Assume 'text' has no HTML escaping.\n",
    "def tokenize(text):\n",
    "    return simpleTokenize(squeezeWhitespace(text))\n",
    "\n",
    "# Twitter text comes HTML-escaped, so unescape it.\n",
    "# We also first unescape &amp;'s, in case the text has been buggily double-escaped.\n",
    "def normalizeTextForTagger(text):\n",
    "    assert sys.version_info[0] >= 3 and sys.version_info[1] > 3, 'Python version >3.3 required'\n",
    "    text = text.replace(\"&amp;\", \"&\")\n",
    "    text = html.unescape(text)\n",
    "    return text\n",
    "\n",
    "# This is intended for raw tweet text -- we do some HTML entity unescaping before running the tagger.\n",
    "#\n",
    "# This function normalizes the input text BEFORE calling the tokenizer.\n",
    "# So the tokens you get back may not exactly correspond to\n",
    "# substrings of the original text.\n",
    "def tokenizeRawTweetText(text):\n",
    "    tokens = tokenize(normalizeTextForTagger(text))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Wrapper for POS Tagger\n",
    "- The functions below call the runTagger.sh to get the POS tag predictions for the tokenized tweets. \n",
    "- runTagger.sh script should be invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TAGGER_CMD = \"java -XX:ParallelGCThreads=2 -Xmx500m -jar ark-tweet-nlp-0.3.2.jar\"\n",
    "\n",
    "def _split_results(rows):\n",
    "    \"\"\"Parse the tab-delimited returned lines, modified from: https://github.com/brendano/ark-tweet-nlp/blob/master/scripts/show.py\"\"\"\n",
    "    for line in rows:\n",
    "        line = line.strip()  # remove '\\n'\n",
    "        if len(line) > 0:\n",
    "            if line.count('\\t') == 2:\n",
    "                parts = line.split('\\t')\n",
    "                tokens = parts[0]\n",
    "                tags = parts[1]\n",
    "                confidence = float(parts[2])\n",
    "                yield tokens, tags, confidence\n",
    "                \n",
    "                \n",
    "def _call_runtagger(tweets, run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Call runTagger.sh using a named input file\"\"\"\n",
    "\n",
    "    # remove carriage returns as they are tweet separators for the stdin\n",
    "    # interface\n",
    "    tweets_cleaned = [tw.replace('\\n', ' ') for tw in tweets]\n",
    "    message = \"\\n\".join(tweets_cleaned)\n",
    "\n",
    "    # force UTF-8 encoding (from internal unicode type) to avoid .communicate encoding error as per:\n",
    "    # http://stackoverflow.com/questions/3040101/python-encoding-for-pipe-communicate\n",
    "    message = message.encode('utf-8')\n",
    "\n",
    "    # build a list of args\n",
    "    args = shlex.split(run_tagger_cmd)\n",
    "    args.append('--output-format')\n",
    "    args.append('conll')\n",
    "    po = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    # old call - made a direct call to runTagger.sh (not Windows friendly)\n",
    "    #po = subprocess.Popen([run_tagger_cmd, '--output-format', 'conll'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    result = po.communicate(message)\n",
    "    # expect a tuple of 2 items like:\n",
    "    # ('hello\\t!\\t0.9858\\nthere\\tR\\t0.4168\\n\\n',\n",
    "    # 'Listening on stdin for input.  (-h for help)\\nDetected text input format\\nTokenized and tagged 1 tweets (2 tokens) in 7.5 seconds: 0.1 tweets/sec, 0.3 tokens/sec\\n')\n",
    "\n",
    "    pos_result = result[0].decode('utf-8').strip('\\n\\n')  # get first line, remove final double carriage return\n",
    "    pos_result = pos_result.split('\\n\\n')  # split messages by double carriage returns\n",
    "    pos_results = [pr.split('\\n') for pr in pos_result]  # split parts of message by each carriage return\n",
    "    return pos_results\n",
    "\n",
    "\n",
    "def runtagger_parse(tweets, run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Call runTagger.sh on a list of tweets, parse the result, return lists of tuples of (term, type, confidence)\"\"\"\n",
    "    pos_raw_results = _call_runtagger(tweets, run_tagger_cmd)\n",
    "    pos_result = []\n",
    "    for pos_raw_result in pos_raw_results:\n",
    "        pos_result.append([x for x in _split_results(pos_raw_result)])\n",
    "    return pos_result\n",
    "\n",
    "\n",
    "def check_script_is_present(run_tagger_cmd=RUN_TAGGER_CMD):\n",
    "    \"\"\"Simple test to make sure we can see the script\"\"\"\n",
    "    success = False\n",
    "    try:\n",
    "        args = shlex.split(run_tagger_cmd)\n",
    "        args.append(\"--help\")\n",
    "        po = subprocess.Popen(args, stdout=subprocess.PIPE)\n",
    "        # old call - made a direct call to runTagger.sh (not Windows friendly)\n",
    "        #po = subprocess.Popen([run_tagger_cmd, '--help'], stdout=subprocess.PIPE)\n",
    "        while not po.poll():\n",
    "            lines = [l for l in po.stdout]\n",
    "        # we expected the first line of --help to look like the following:\n",
    "        assert \"RunTagger [options]\" in lines[0].decode('utf-8')\n",
    "        success = True\n",
    "    except OSError as err:\n",
    "        print(\"Caught an OSError, have you specified the correct path to runTagger.sh? We are using \\\"%s\\\". Exception: %r\" % (run_tagger_cmd, repr(err)))\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read tokenized tweets\n",
    "We will now load tweets that have the tokenized for POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I won't win a single game I bet on !! Got Mr. Cliff Lee , if he loses its on me U.S.A ! $ 5 0.0 .\\n\", '@thecamion I like monkeys , but I still hate COSTCO parking lots .. oO o.O #COSTCO 2:15 PM\\n', 'RT @eye_e : this poster-print costs $ 12 . 40 , which is 40% of the normal price ! http://tl.gd/6meogh\\n', 'LMBO ! This man filed an EMERGENCY Motion for Continuance on account of the Rangers game tonight !\\n', 'Texas Rangers are in the World Series ! Go Rangers !!!!!!!!! : > <3 ♥❤♡ http://fb.me/D2LsXBJx\\n']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"tweets_tokenized.txt\", \"r\")\n",
    "tweets_tokenized = file.readlines()\n",
    "print(tweets_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply POS tagger\n",
    "The output of the POS tagger is a tuple containing token, predicted output tag, and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: I won't win a single game I bet on!! Got Mr. Cliff Lee, if he loses its on me U.S.A!\n",
      "\n",
      "I won't win a single          game I bet on!! Got Mr. Cliff Lee, if he loses its on me U.S.A!\tI won't win a single game I bet on!! Got Mr. Cliff Lee, if he loses its on me U.S.A!\t[('I', 'O', 0.9942), (\"won't\", 'V', 0.9982), ('win', 'V', 0.9993), ('a', 'D', 0.9987), ('single', 'A', 0.9758), ('game', 'N', 0.9988), ('I', 'O', 0.9922), ('bet', 'V', 0.9995), ('on', 'P', 0.9162), ('!!', ',', 0.9873), ('Got', 'V', 0.9965), ('Mr.', '^', 0.9863), ('Cliff', '^', 0.9993), ('Lee', '^', 0.9999), (',', ',', 0.9975), ('if', 'P', 0.9987), ('he', 'O', 0.9979), ('loses', 'V', 0.9996), ('its', 'L', 0.9865), ('on', 'P', 0.9871), ('me', 'O', 0.9862), ('U.S.A', '^', 0.7116), ('!', ',', 0.9927)]\n",
      "\n",
      "t: RT @eye_e: this poster-print costs $12.40 , which is 40% of the normal price! http://tl.gd/6meogh\n",
      "\n",
      "RT @eye_e: this poster-print costs $12.40, which is 40% of the normal price! http://tl.gd/6meogh\tRT @eye_e: this poster-print costs $12.40 , which is 40% of the normal price! http://tl.gd/6meogh\t[('RT', '~', 0.9979), ('@eye_e', '@', 0.9992), (':', '~', 0.9756), ('this', 'D', 0.9815), ('poster-print', 'N', 0.9922), ('costs', 'V', 0.5971), ('$12.40', '$', 0.9882), (',', ',', 0.9973), ('which', 'O', 0.6752), ('is', 'V', 0.9986), ('40%', '$', 0.9843), ('of', 'P', 0.9985), ('the', 'D', 0.9996), ('normal', 'A', 0.9925), ('price', 'N', 0.9989), ('!', ',', 0.9987), ('http://tl.gd/6meogh', 'U', 0.9948)]\n",
      "\n",
      "t: I ❤ Biebs & want to hang out with him!!\n",
      "\n",
      "I ❤ Biebs & want to hang out with him!!\tI ❤ Biebs & want to hang out with him!!\t[('I', 'O', 0.9939), ('❤', 'V', 0.474), ('Biebs', '^', 0.5272), ('&', '&', 0.9905), ('want', 'V', 0.9952), ('to', 'P', 0.9946), ('hang', 'V', 0.9998), ('out', 'T', 0.9532), ('with', 'P', 0.9994), ('him', 'O', 0.9775), ('!!', ',', 0.9958)]\n",
      "\n",
      "t: @thecamion I like monkeys, but I still hate COSTCO parking lots.. oO o.O #COSTCO 2:15 PM\n",
      "\n",
      "@thecamion I like monkeys, but I still hate COSTCO parking lots.. oO o.O #COSTCO 2:15PM\t@thecamion I like monkeys, but I still hate COSTCO parking lots.. oO o.O #COSTCO 2:15 PM\t[('@thecamion', '@', 0.9995), ('I', 'O', 0.9953), ('like', 'V', 0.9027), ('monkeys', 'N', 0.9408), (',', ',', 0.999), ('but', '&', 0.9974), ('I', 'O', 0.9988), ('still', 'R', 0.9858), ('hate', 'V', 0.9938), ('COSTCO', '^', 0.953), ('parking', 'N', 0.6863), ('lots', 'N', 0.9851), ('..', ',', 0.9939), ('oO', '!', 0.9601), ('o.O', 'E', 0.7939), ('#COSTCO', '^', 0.5895), ('2:15', '$', 0.9761), ('PM', 'N', 0.9566)]\n",
      "\n",
      "t: Texas Rangers are in the World Series! Go Rangers!!!!!!!!! :> <3 ♥❤♡ http://fb.me/D2LsXBJx\n",
      "\n",
      "Texas Rangers are in the World Series!  Go Rangers!!!!!!!!! :> <3 ♥❤♡ http://fb.me/D2LsXBJx\tTexas Rangers are in the World Series! Go Rangers!!!!!!!!! :> <3 ♥❤♡ http://fb.me/D2LsXBJx\t[('Texas', '^', 0.9984), ('Rangers', '^', 0.9582), ('are', 'V', 0.9913), ('in', 'P', 0.9947), ('the', 'D', 0.999), ('World', '^', 0.8136), ('Series', '^', 0.8281), ('!', ',', 0.9896), ('Go', 'V', 0.987), ('Rangers', '^', 0.9369), ('!!!!!!!!!', ',', 0.9808), (':', ',', 0.7083), ('>', 'E', 0.6507), ('<3', 'E', 0.998), ('♥❤♡', 'E', 0.5116), ('http://fb.me/D2LsXBJx', 'U', 0.988)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_text = 'tweets.txt'\n",
    "oup_file = open(\"tweets_pos.txt\", \"w\") \n",
    "inp_file = open('tweets.txt')\n",
    "#oup_file = open(\"tweets_tokenized.txt\", \"w\") \n",
    "for line in inp_file:\n",
    "    tokenized_tweet = ' '.join(tokenizeRawTweetText(line))\n",
    "    out = runtagger_parse([tokenized_tweet])\n",
    "    #tokenized_tweet = ' '.join(simpleTokenize(line))\n",
    "    print('t: ' + tokenized_tweet + '\\n')\n",
    "    print(line.strip()+'\\t'+tokenized_tweet+'\\t'+str(out[0]) + '\\n')\n",
    "    oup_file.write(line.strip()+'\\t'+tokenized_tweet+'\\t'+str(out[0]) + '\\n')\n",
    "inp_file.close()\n",
    "oup_file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
