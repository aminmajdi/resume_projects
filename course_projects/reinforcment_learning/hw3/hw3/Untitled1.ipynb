{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II\n",
    "Answer the following questions:\n",
    "1. What action does your optimal policy suggest for capital of 50? What about for capital of 51?\n",
    "> Answer:\n",
    "<br>$p_h = 0.25 \\rightarrow \\pi^*(50)=50 ,\\pi^*(51)=1 $\n",
    "<br>$p_h = 0.4 \\rightarrow \\pi^*(50)=50 ,\\pi^*(51)=1$\n",
    "<br>$p_h = 0.55 \\rightarrow \\pi^*(50)=1 ,\\pi^*(51)=1$\n",
    "\n",
    "2. Why do you think your optimal policy is a good policy? Explain.\n",
    "> Answer:\n",
    "when   $p_h = 0.25$ and $p_h = 0.4$ , $p_h <(1-p_h)$ so the value of lower state(v(s-a)) in comparison to value of higher state (v(s+a)) has more weight on v(s). so in the long run best action is reducing the number of bets and the best way to do this is going to shortcut states like 25,50,75 and so on. So we see that best action for state 76 is to go back to the shortcut 75 to go eather 100 or 50. \n",
    "<br>On the other hand for $p_h >(1-p_h)$ like $p_h = 0.55$ as probability of win is more than loose the algorithm choose action 1 for all states. In this policy chance of going to upper states is higher than lower states so the algorithm prefers to play more to be sure that at the end it will see the goal with the highest probability. in the long run all state values upper than 25 equals to 1. which means that player almost always wins with action=1, but (for example) if player bets max, with probability of $(1-p_h)$ for state=[1-50]and  $(1-p_h)^2$ for state=[51-99] he looses the game. thats why the algorithm is plays good and optimal. \n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III*\n",
    "Test the algorithm by decreasing $\\theta$ the threshold for accuracy of value function estimation. What happens when $\\theta \\rightarrow 0$? You can add any helpful code/graphs if you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer: After testing algorithm for all three 'p_h's it is clear that for smaller thresholds, number of sweeps increases and each time the state values will be closer and closer to optimal state values. also after each sweep  final policy will be more robust and will be so close to optimal policy.finally for theta = 0.0001 all the policies and all the state values will be stady and after that decreasing the theta only couse more useless process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
