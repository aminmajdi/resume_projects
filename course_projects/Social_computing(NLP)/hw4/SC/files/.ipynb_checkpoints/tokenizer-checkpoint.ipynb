{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Tokenizer\n",
    "\n",
    "Tokenization is the process of separating out or segmenting words in a given text. This is often one of the first steps while working with text. In English, words are often separated from each other by whitespace, but whitespace is not always sufficient:\n",
    "\n",
    "- 'Los Angeles' or 'self sustaining' in \"a company in Los Angeles is building a self sustaining city on Mars\" are treated as large words despite the fact that they contain spaces.\n",
    "- Sometimes we need to separate *I'm* into the two words 'I' and 'am.'\n",
    "- Sometimes hyphenated words that should be separated, e.g. \"the hold-him-back-and-drag-him-away maneuver\"\n",
    "- Sometimes sentences carry words with missing or spurious spaces, e.g. \"wer an after him qui ckly\" which should yield {we, ran, after, him, quickly}\n",
    "- Sometimes unusual tokens should be recognized as single tokens, e.g., the TV series 'M\\*A\\*S\\*H.'\n",
    "\n",
    "The goal of this tutorial is to introduce a tokenizer developed for tweets which was released as part of the [TweetNLP](https://www.ark.cs.cmu.edu/TweetNLP/) toolkit. The code is written in Java and the python version for the tokenization is from [this](https://github.com/myleott/ark-twokenize-py) github repository with slight modifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the Big Deal?\n",
    "Attempting to tokenize the tweets without separating out punctuations, emoticons, etc as described earlier gives something like this. Note that most of the punctuation and special characters are not separated out which poses a problem during tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o: I won't win a single          game I bet on!! Got Mr. Cliff Lee, if he loses its on me U.S.A!\n",
      "t:  ['I', \"won't\", 'win', 'a', 'single', '', '', '', '', '', '', '', '', '', 'game', 'I', 'bet', 'on!!', 'Got', 'Mr.', 'Cliff', 'Lee,', 'if', 'he', 'loses', 'its', 'on', 'me', 'U.S.A!\\n'] \n",
      "\n",
      "o: RT @eye_e: this poster-print costs $12.40, which is 40% of the normal price! http://tl.gd/6meogh\n",
      "t:  ['RT', '@eye_e:', 'this', 'poster-print', 'costs', '$12.40,', 'which', 'is', '40%', 'of', 'the', 'normal', 'price!', 'http://tl.gd/6meogh\\n'] \n",
      "\n",
      "o: I ❤ Biebs & want to hang out with him!!\n",
      "t:  ['I', '❤', 'Biebs', '&', 'want', 'to', 'hang', 'out', 'with', 'him!!\\n'] \n",
      "\n",
      "o: @thecamion I like monkeys, but I still hate COSTCO parking lots.. oO o.O #COSTCO 2:15PM\n",
      "t:  ['@thecamion', 'I', 'like', 'monkeys,', 'but', 'I', 'still', 'hate', 'COSTCO', 'parking', 'lots..', 'oO', 'o.O', '#COSTCO', '2:15PM\\n'] \n",
      "\n",
      "o: Texas Rangers are in the World Series!  Go Rangers!!!!!!!!! :> <3 ♥❤♡ http://fb.me/D2LsXBJx\n",
      "t:  ['Texas', 'Rangers', 'are', 'in', 'the', 'World', 'Series!', '', 'Go', 'Rangers!!!!!!!!!', ':>', '<3', '♥❤♡', 'http://fb.me/D2LsXBJx\\n'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_text = 'tweets.txt'\n",
    "inp_file = open(raw_text)\n",
    "for line in inp_file:\n",
    "    print('o: ' + line.strip())\n",
    "    tokenized_tweet = line.split(' ')\n",
    "    print('t: ', tokenized_tweet, '\\n')\n",
    "inp_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background on Regular Expressions \n",
    "\n",
    "Regular expressions or regex is used for for specifying patterns to search text strings\n",
    "- For an indepth intro into regex you can see the following resource: https://web.stanford.edu/~jurafsky/slp3/2.pdf\n",
    "- See https://docs.python.org/2/library/re.html for syntax regarding the different operations.\n",
    "\n",
    "#### Important Notes:\n",
    "- Placing an 'r' before a string indicates that the string needs to be interpreted as a raw string. So, r\"\\n\" is a two-character string containing '\\' and 'n', while \"\\n\" is a one-character string containing a newline. This is to avoid colliding with Python's usage of the same character for the same purpose in string literals.\n",
    "- Characters in a set that require to be matched are indicated by brackets []. For example [amk] will match a, m, or k.\n",
    "- (?iLmsux) (One or more letters from the set 'i', 'L', 'm', 's', 'u', 'x'.) The group matches the empty string; the letters set the corresponding flags: re.I (ignore case), re.L (locale dependent), re.M (multi-line), re.S (dot matches all), re.U (Unicode dependent), and re.X (verbose), for the entire regular expression. This is useful if you wish to include the flags as part of the regular expression, instead of passing a flag argument to the re.compile() function. \n",
    "- \\u00a0\\u1680\\u2000-\\u200a\\u2028\\u2029\\u202f\\u205f\\u3000\\ufeff refer to different white spaces. For a detailed description of what these unicode values mean you can check out this resource: http://www.endmemo.com/unicode/ascii.php\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import operator\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "import tokenize\n",
    "try:\n",
    "    from html.parser import HTMLParser\n",
    "except ImportError:\n",
    "    from HTMLParser import HTMLParser\n",
    "    \n",
    "\n",
    "try:\n",
    "    import html\n",
    "except ImportError:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### whitespaces and contractions\n",
    "\n",
    "- Contractions are shortened version of words or syllables, e.g., expressions such as n't, I'd, etc. See list of English contractions [here](https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Contractions = re.compile(u\"(?i)(\\w+)(n['’′]t|['’′]ve|['’′]ll|['’′]d|['’′]re|['’′]s|['’′]m)$\", re.UNICODE)\n",
    "Whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation characters and sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctChars = r\"['\\\"“”‘’.?!…,:;]\"\n",
    "punctSeq   = r\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\n",
    "entity     = r\"&(?:amp|lt|gt|quot);\" # see more here https://www.w3schools.com/html/html_entities.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining multiple expressions\n",
    "Let's defin the regex_or function to join multiple regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_or(*items):\n",
    "    return '(?:' + '|'.join(items) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLs & country domains\n",
    "- (?: pattern ): Occasionally we might want to use parentheses for grouping, but don’t want to capture the resulting pattern in a register. In that case we use a non-capturing group, which is specified by putting the commands ?: after the open paren, in the form (?: pattern ). It matches the pattern inside the parentheses, but the substring matched by the group cannot be retrieved after performing a match or referenced later in the pattern. For example, *(?:some|a few) (people|cats) like some \\1* will match \"some cats like some cats\" but not \"some cats like some a few\".\n",
    "- (?= pattern) : This operator is true if pattern occurs, but is zero-width, i.e. the match pointer doesn’t dvance. The operator\n",
    "- (?! pattern): it only returns true if a pattern does not match, but again is zero-width and doesn’t advance the cursor. For example suppose we want to match, at the beginning of a line, any single word that doesn’t start with “Volcano”. We can use negative lookahead to do this: *ˆ(?!Volcano)[A-Za-z]+*\n",
    "- (?(id/name)yes-pattern|no-pattern): will try to match with yes-pattern if the group with given id or name exists, and with no-pattern if it doesn’t. no-pattern is optional and can be omitted. For example, (<)?(\\w+@\\w+(?:\\.\\w+)+)(?(1)>) is a poor email matching pattern, which will match with '\\<user@host.com\\>' as well as 'user@host.com', but not with '\\<user@host.com'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlStart1  = r\"(?:https?://|\\bwww\\.)\"\n",
    "commonTLDs = r\"(?:com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xxx)\"\n",
    "ccTLDs = r\"(?:ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|\" + \\\n",
    "r\"bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|\" + \\\n",
    "r\"er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|\" + \\\n",
    "r\"hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|\" + \\\n",
    "r\"lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|\" + \\\n",
    "r\"nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|\" + \\\n",
    "r\"sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|\" + \\\n",
    "r\"va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)\"\t#TODO: remove obscure country domains?\n",
    "urlStart2  = r\"\\b(?:[A-Za-z\\d-])+(?:\\.[A-Za-z0-9]+){0,3}\\.\" + regex_or(commonTLDs, ccTLDs) + r\"(?:\\.\"+ccTLDs+r\")?(?=\\W|$)\"\n",
    "urlBody    = r\"(?:[^\\.\\s<>][^\\s<>]*?)?\"\n",
    "urlExtraCrapBeforeEnd = regex_or(punctChars, entity) + \"+?\"\n",
    "urlEnd     = r\"(?:\\.\\.+|[<>]|\\s|$)\"\n",
    "url        = regex_or(urlStart1, urlStart2) + urlBody + \"(?=(?:\"+urlExtraCrapBeforeEnd+\")?\"+urlEnd+\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric\n",
    "Regular expressions to identify numeric characters in time, separated by commas, and number combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "monetary = r\"\\$([0-9]+)?\\.?([0-9]+)?\"\n",
    "timeLike   = r\"\\d+(?::\\d+){1,2}\"\n",
    "numberWithCommas = r\"(?:(?<!\\d)\\d{1,3},)+?\\d{3}\" + r\"(?=(?:[^,\\d]|$))\"\n",
    "numComb = u\"[\\u0024\\u058f\\u060b\\u09f2\\u09f3\\u09fb\\u0af1\\u0bf9\\u0e3f\\u17db\\ua838\\ufdfc\\ufe69\\uff04\\uffe0\\uffe1\\uffe5\\uffe6\\u00a2-\\u00a5\\u20a0-\\u20b9]?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abbreviations\n",
    "Regular expression for abbreviations like Mr., Mrs. etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaryNotDot = regex_or(\"$\", r\"\\s\", r\"[“\\\"?!,:;]\", entity)\n",
    "aa1  = r\"(?:[A-Za-z]\\.){2,}(?=\" + boundaryNotDot + \")\"\n",
    "aa2  = r\"[^A-Za-z](?:[A-Za-z]\\.){1,}[A-Za-z](?=\" + boundaryNotDot + \")\"\n",
    "standardAbbreviations = r\"\\b(?:[Mm]r|[Mm]rs|[Mm]s|[Dd]r|[Ss]r|[Jj]r|[Rr]ep|[Ss]en|[Ss]t)\\.\"\n",
    "arbitraryAbbrev = regex_or(aa1, aa2, standardAbbreviations)\n",
    "separators  = \"(?:--+|―|—|~|–|=)\"\n",
    "decorations = u\"(?:[♫♪]+|[★☆]+|[♥❤♡]+|[\\u2639-\\u263b]+|[\\ue001-\\uebbb]+)\"\n",
    "thingsThatSplitWords = r\"[^\\s\\.,?\\\"]\"\n",
    "embeddedApostrophe = thingsThatSplitWords+r\"+['’′]\" + thingsThatSplitWords + \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoticons\n",
    "Regular expression for emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalEyes = \"[:=]\" # 8 and x are eyes but cause problems\n",
    "wink = \"[;]\"\n",
    "noseArea = \"(?:|-|[^a-zA-Z0-9 ])\" # doesn't get :'-(\n",
    "happyMouths = r\"[D\\)\\]\\}]+\"\n",
    "sadMouths = r\"[\\(\\[\\{]+\"\n",
    "tongue = \"[pPd3]+\"\n",
    "otherMouths = r\"(?:[oO]+|[/\\\\]+|[vV]+|[Ss]+|[|]+)\" # remove forward slash if http://'s aren't cleaned\n",
    "\n",
    "# mouth repetition examples:\n",
    "# @aliciakeys Put it in a love song :-))\n",
    "# @hellocalyclops =))=))=)) Oh well\n",
    "\n",
    "# myleott: try to be as case insensitive as possible, but still not perfect, e.g., o.O fails\n",
    "#bfLeft = u\"(♥|0|o|°|v|\\\\$|t|x|;|\\u0ca0|@|ʘ|•|・|◕|\\\\^|¬|\\\\*)\".encode('utf-8')\n",
    "bfLeft = u\"(♥|0|[oO]|°|[vV]|\\\\$|[tT]|[xX]|;|\\u0ca0|@|ʘ|•|・|◕|\\\\^|¬|\\\\*)\"\n",
    "bfCenter = r\"(?:[\\.]|[_-]+)\"\n",
    "bfRight = r\"\\2\"\n",
    "s3 = r\"(?:--['\\\"])\"\n",
    "s4 = r\"(?:<|&lt;|>|&gt;)[\\._-]+(?:<|&lt;|>|&gt;)\"\n",
    "s5 = \"(?:[.][_]+[.])\"\n",
    "# myleott: in Python the (?i) flag affects the whole expression\n",
    "#basicface = \"(?:(?i)\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "basicface = \"(?:\" +bfLeft+bfCenter+bfRight+ \")|\" +s3+ \"|\" +s4+ \"|\" + s5\n",
    "\n",
    "eeLeft = r\"[＼\\\\ƪԄ\\(（<>;ヽ\\-=~\\*]+\"\n",
    "eeRight= u\"[\\\\-=\\\\);'\\u0022<>ʃ）/／ノﾉ丿╯σっµ~\\\\*]+\"\n",
    "eeSymbol = r\"[^A-Za-z0-9\\s\\(\\)\\*:=-]\"\n",
    "eastEmote = eeLeft + \"(?:\"+basicface+\"|\" +eeSymbol+\")+\" + eeRight\n",
    "\n",
    "oOEmote = r\"(?:[oO]\" + bfCenter + r\"[oO])\"\n",
    "\n",
    "emoticon = regex_or(\n",
    "        # Standard version  :) :( :] :D :P\n",
    "        \"(?:>|&gt;)?\" + regex_or(normalEyes, wink) + regex_or(noseArea,\"[Oo]\") + regex_or(tongue+r\"(?=\\W|$|RT|rt|Rt)\", otherMouths+r\"(?=\\W|$|RT|rt|Rt)\", sadMouths, happyMouths),\n",
    "\n",
    "        # reversed version (: D:  use positive lookbehind to remove \"(word):\"\n",
    "        # because eyes on the right side is more ambiguous with the standard usage of : ;\n",
    "        regex_or(\"(?<=(?: ))\", \"(?<=(?:^))\") + regex_or(sadMouths,happyMouths,otherMouths) + noseArea + regex_or(normalEyes, wink) + \"(?:<|&lt;)?\",\n",
    "\n",
    "        #inspired by http://en.wikipedia.org/wiki/User:Scapler/emoticons#East_Asian_style\n",
    "        eastEmote.replace(\"2\", \"1\", 1), basicface,\n",
    "        # iOS 'emoji' characters (some smileys, some symbols) [\\ue001-\\uebbb]\n",
    "        # TODO should try a big precompiled lexicon from Wikipedia, Dan Ramage told me (BTO) he does this\n",
    "\n",
    "        # myleott: o.O and O.o are two of the biggest sources of differences\n",
    "        #          between this and the Java version. One little hack won't hurt...\n",
    "        oOEmote\n",
    ")\n",
    "\n",
    "Hearts = \"(?:<+/?3+)+\" #the other hearts are in decorations\n",
    "\n",
    "Arrows = regex_or(r\"(?:<*[-―—=]*>+|<+[-―—=]*>*)\", u\"[\\u2190-\\u21ff]+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Specific Characters\n",
    "\n",
    "Regular expressions for twitter specific characters such as hashtags, at mentions, email. Details for regular expressions for emails can be found here : http://www.regular-expressions.info/email.html\n",
    "Note: \n",
    "- When the LOCALE and UNICODE flags are not specified, [\\w] matches any alphanumeric character and the underscore; this is equivalent to the set [a-zA-Z0-9_]. With LOCALE, it will match the set [0-9_] plus whatever characters are defined as alphanumeric for the current locale. If UNICODE is set, this will match the characters [0-9_] plus whatever is classified as alphanumeric in the Unicode character properties database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hashtag = \"#[a-zA-Z0-9_]+\"\n",
    "AtMention = \"[@＠][a-zA-Z0-9_]+\"\n",
    "\n",
    "Bound = r\"(?:\\W|^|$)\"\n",
    "Email = regex_or(\"(?<=(?:\\W))\", \"(?<=(?:^))\") + r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}(?=\" +Bound+\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Together \n",
    "\n",
    "Create a regex object with all the above mentioned search patterns and identify edge punctuation - this includes punctuations in the edges of words (e.g., 'foo' to become ' foo ') but not in the middle of words like don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be tokenizing using these regexps as delimiters\n",
    "# Additionally, these things are \"protected\", meaning they shouldn't be further split themselves.\n",
    "Protected  = re.compile(\n",
    "    regex_or(\n",
    "        Hearts,\n",
    "        url,\n",
    "        Email,\n",
    "        timeLike,\n",
    "        monetary,\n",
    "        numberWithCommas,\n",
    "        numComb,\n",
    "        emoticon,\n",
    "        Arrows,\n",
    "        entity,\n",
    "        punctSeq,\n",
    "        arbitraryAbbrev,\n",
    "        separators,\n",
    "        decorations,\n",
    "        embeddedApostrophe,\n",
    "        Hashtag,\n",
    "        AtMention), re.UNICODE)\n",
    "\n",
    "# Edge punctuation\n",
    "# Want: 'foo' => ' foo '\n",
    "# While also:   don't => don't\n",
    "# the first is considered \"edge punctuation\".\n",
    "# the second is word-internal punctuation -- don't want to mess with it.\n",
    "# BTO (2011-06): the edgepunct system seems to be the #1 source of problems these days.\n",
    "# I remember it causing lots of trouble in the past as well.  Would be good to revisit or eliminate.\n",
    "\n",
    "# Note the 'smart quotes' (http://en.wikipedia.org/wiki/Smart_quotes)\n",
    "#edgePunctChars    = r\"'\\\"“”‘’«»{}\\(\\)\\[\\]\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunctChars    = u\"'\\\"“”‘’«»{}\\\\(\\\\)\\\\[\\\\]\\\\*&\" #add \\\\p{So}? (symbols)\n",
    "edgePunct    = \"[\" + edgePunctChars + \"]\"\n",
    "notEdgePunct = \"[a-zA-Z0-9]\" # content characters\n",
    "offEdge = r\"(^|$|:|;|\\s|\\.|,)\"  # colon here gets \"(hello):\" ==> \"( hello ):\"\n",
    "EdgePunctLeft  = re.compile(offEdge + \"(\"+edgePunct+\"+)(\"+notEdgePunct+\")\", re.UNICODE)\n",
    "EdgePunctRight = re.compile(\"(\"+notEdgePunct+\")(\"+edgePunct+\"+)\" + offEdge, re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "The functions here help in the tokenization of the input tweets, where each token needs to be separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitEdgePunct(input):\n",
    "    input = EdgePunctLeft.sub(r\"\\1\\2 \\3\", input)\n",
    "    input = EdgePunctRight.sub(r\"\\1 \\2\\3\", input)\n",
    "    return input\n",
    "\n",
    "# The main work of tokenizing a tweet.\n",
    "def simpleTokenize(text):\n",
    "\n",
    "    # Do the no-brainers first\n",
    "    splitPunctText = splitEdgePunct(text)\n",
    "\n",
    "    textLength = len(splitPunctText)\n",
    "\n",
    "    # BTO: the logic here got quite convoluted via the Scala porting detour\n",
    "    # It would be good to switch back to a nice simple procedural style like in the Python version\n",
    "    # ... Scala is such a pain.  Never again.\n",
    "\n",
    "    # Find the matches for subsequences that should be protected,\n",
    "    # e.g. URLs, 1.0, U.N.K.L.E., 12:53\n",
    "    bads = []\n",
    "    badSpans = []\n",
    "    for match in Protected.finditer(splitPunctText):\n",
    "        # The spans of the \"bads\" should not be split.\n",
    "        if (match.start() != match.end()): #unnecessary?\n",
    "            bads.append( [splitPunctText[match.start():match.end()]] )\n",
    "            badSpans.append( (match.start(), match.end()) )\n",
    "\n",
    "    # Create a list of indices to create the \"goods\", which can be\n",
    "    # split. We are taking \"bad\" spans like\n",
    "    #     List((2,5), (8,10))\n",
    "    # to create\n",
    "    #     List(0, 2, 5, 8, 10, 12)\n",
    "    # where, e.g., \"12\" here would be the textLength\n",
    "    # has an even length and no indices are the same\n",
    "    indices = [0]\n",
    "    for (first, second) in badSpans:\n",
    "        indices.append(first)\n",
    "        indices.append(second)\n",
    "    indices.append(textLength)\n",
    "\n",
    "    # Group the indices and map them to their respective portion of the string\n",
    "    splitGoods = []\n",
    "    for i in range(0, len(indices), 2):\n",
    "        goodstr = splitPunctText[indices[i]:indices[i+1]]\n",
    "        splitstr = goodstr.strip().split(\" \")\n",
    "        splitGoods.append(splitstr)\n",
    "\n",
    "    #  Reinterpolate the 'good' and 'bad' Lists, ensuring that\n",
    "    #  additonal tokens from last good item get included\n",
    "    zippedStr = []\n",
    "    for i in range(len(bads)):\n",
    "        zippedStr = addAllnonempty(zippedStr, splitGoods[i])\n",
    "        zippedStr = addAllnonempty(zippedStr, bads[i])\n",
    "    zippedStr = addAllnonempty(zippedStr, splitGoods[len(bads)])\n",
    "\n",
    "    # BTO: our POS tagger wants \"ur\" and \"you're\" to both be one token.\n",
    "    # Uncomment to get \"you 're\"\n",
    "    #splitStr = []\n",
    "    #for tok in zippedStr:\n",
    "    #    splitStr.extend(splitToken(tok))\n",
    "    #zippedStr = splitStr\n",
    "\n",
    "    return zippedStr\n",
    "\n",
    "\n",
    "def addAllnonempty(master, smaller):\n",
    "    for s in smaller:\n",
    "        strim = s.strip()\n",
    "        if (len(strim) > 0):\n",
    "            master.append(strim)\n",
    "    return master\n",
    "\n",
    "# \"foo   bar \" => \"foo bar\"\n",
    "def squeezeWhitespace(input):\n",
    "    return Whitespace.sub(\" \", input).strip()\n",
    "\n",
    "# Final pass tokenization based on special patterns\n",
    "def splitToken(token):\n",
    "    m = Contractions.search(token)\n",
    "    if m:\n",
    "        return [m.group(1), m.group(2)]\n",
    "    return [token]\n",
    "\n",
    "# Assume 'text' has no HTML escaping.\n",
    "def tokenize(text):\n",
    "    return simpleTokenize(squeezeWhitespace(text))\n",
    "\n",
    "# Twitter text comes HTML-escaped, so unescape it.\n",
    "# We also first unescape &amp;'s, in case the text has been buggily double-escaped.\n",
    "def normalizeTextForTagger(text):\n",
    "    assert sys.version_info[0] >= 3 and sys.version_info[1] > 3, 'Python version >3.3 required'\n",
    "    text = text.replace(\"&amp;\", \"&\")\n",
    "    text = html.unescape(text)\n",
    "    return text\n",
    "\n",
    "# This is intended for raw tweet text -- we do some HTML entity unescaping before running the tagger.\n",
    "#\n",
    "# This function normalizes the input text BEFORE calling the tokenizer.\n",
    "# So the tokens you get back may not exactly correspond to\n",
    "# substrings of the original text.\n",
    "def tokenizeRawTweetText(text):\n",
    "    tokens = tokenize(normalizeTextForTagger(text))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run\n",
    "The folder comes with example_tweets.txt, these contain the raw tweets and need to be tokenized before attempting to do the POS tagging. Once the tweets are tokenized we can store it in a file Tokenized.txt so that the pos tagger can directly read from this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o: I won't win a single          game I bet on!! Got Mr. Cliff Lee, if he loses its on me U.S.A!\n",
      "t: I won't win a single game I bet on !! Got Mr. Cliff Lee , if he loses its on me U.S.A !\n",
      "\n",
      "o: RT @eye_e: this poster-print costs $12.40, which is 40% of the normal price! http://tl.gd/6meogh\n",
      "t: RT @eye_e : this poster-print costs $12.40 , which is 40% of the normal price ! http://tl.gd/6meogh\n",
      "\n",
      "o: I ❤ Biebs & want to hang out with him!!\n",
      "t: I ❤ Biebs & want to hang out with him !!\n",
      "\n",
      "o: @thecamion I like monkeys, but I still hate COSTCO parking lots.. oO o.O #COSTCO 2:15PM\n",
      "t: @thecamion I like monkeys , but I still hate COSTCO parking lots .. oO o.O #COSTCO 2:15 PM\n",
      "\n",
      "o: Texas Rangers are in the World Series!  Go Rangers!!!!!!!!! :> <3 ♥❤♡ http://fb.me/D2LsXBJx\n",
      "t: Texas Rangers are in the World Series ! Go Rangers !!!!!!!!! : > <3 ♥❤♡ http://fb.me/D2LsXBJx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_text = 'tweets.txt'\n",
    "\n",
    "inp_file = open(raw_text)\n",
    "oup_file = open(\"tweets_tokenized.txt\", \"w\") \n",
    "for line in inp_file:\n",
    "    print('o: ' + line.strip())\n",
    "    tokenized_tweet = ' '.join(tokenizeRawTweetText(line))    \n",
    "    print('t: ' + tokenized_tweet + '\\n')\n",
    "    oup_file.write(tokenized_tweet + '\\n')\n",
    "inp_file.close()\n",
    "oup_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
